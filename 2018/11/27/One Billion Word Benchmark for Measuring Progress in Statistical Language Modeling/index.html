<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-32x32.ico?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="baidu-site-verification" content="eYmWT0dEmt">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":true,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="One Billion Word Benchmark for Measuring Progress in Statistical Language ModelingCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson(Submitted on 11 De">
<meta name="keywords" content="Language Modeling">
<meta property="og:type" content="article">
<meta property="og:title" content="待读One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling">
<meta property="og:url" content="https://yuanxiaosc.github.io/2018/11/27/One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling/index.html">
<meta property="og:site_name" content="望江人工智库">
<meta property="og:description" content="One Billion Word Benchmark for Measuring Progress in Statistical Language ModelingCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson(Submitted on 11 De">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2018-12-25T12:04:01.093Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="待读One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling">
<meta name="twitter:description" content="One Billion Word Benchmark for Measuring Progress in Statistical Language ModelingCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson(Submitted on 11 De">
  <link rel="canonical" href="https://yuanxiaosc.github.io/2018/11/27/One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>待读One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling | 望江人工智库</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?359fbde2215e8ede98cdd58478ab2c53";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">望江人工智库</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">TF-KMP</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="fa fa-search fa-fw"></i>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/yuanxiaosc" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yuanxiaosc.github.io/2018/11/27/One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="袁宵">
      <meta itemprop="description" content="专注于人工智能领域研究，特别是自然语言处理。">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="望江人工智库">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">待读One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling

          
        </h2>

        <div class="post-meta">
		  	  
			  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
			   

              
                
              

              <time title="创建时间：2018-11-27 10:30:00" itemprop="dateCreated datePublished" datetime="2018-11-27T10:30:00+08:00">2018-11-27</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2018-12-25 20:04:01" itemprop="dateModified" datetime="2018-12-25T20:04:01+08:00">2018-12-25</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文/" itemprop="url" rel="index"><span itemprop="name">论文</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文/论文阅读/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="One-Billion-Word-Benchmark-for-Measuring-Progress-in-Statistical-Language-Modeling"><a href="#One-Billion-Word-Benchmark-for-Measuring-Progress-in-Statistical-Language-Modeling" class="headerlink" title="One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling"></a><a href="https://arxiv.org/abs/1312.3005v3" target="_blank" rel="noopener">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</a></h1><p>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson<br>(Submitted on 11 Dec 2013 (v1), last revised 4 Mar 2014 (this version, v3))</p><a id="more"></a>
<blockquote>
<p>We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline.<br>The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.</p>
</blockquote>
<p>Comments:    Accompanied by a code.google.com project allowing anyone to generate the benchmark data, and use it to compare their language model against the ones described in the paper<br>Subjects:    Computation and Language (cs.CL)<br>Cite as:    arXiv:1312.3005 [cs.CL]<br>     (or arXiv:1312.3005v3 [cs.CL] for this version)</p>
<h1 id="Language-Model-on-One-Billion-Word-Benchmark"><a href="#Language-Model-on-One-Billion-Word-Benchmark" class="headerlink" title="Language Model on One Billion Word Benchmark"></a><a href="https://github.com/tensorflow/models/tree/master/research/lm_1b" target="_blank" rel="noopener">Language Model on One Billion Word Benchmark</a></h1><p><b>Authors:</b></p>
<p>Oriol Vinyals (vinyals@google.com, github: OriolVinyals),<br>Xin Pan</p>
<p><b>Paper Authors:</b></p>
<p>Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu</p>
<p><b>TL;DR</b></p>
<p>This is a pretrained model on One Billion Word Benchmark.<br>If you use this model in your publication, please cite the original paper:</p>
<p>@article{jozefowicz2016exploring,<br>  title={Exploring the Limits of Language Modeling},<br>  author={Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike<br>          and Shazeer, Noam and Wu, Yonghui},<br>  journal={arXiv preprint arXiv:1602.02410},<br>  year={2016}<br>}</p>
<p><b>Introduction</b></p>
<p>In this release, we open source a model trained on the One Billion Word<br>Benchmark (<a href="http://arxiv.org/abs/1312.3005" target="_blank" rel="noopener">http://arxiv.org/abs/1312.3005</a>), a large language corpus in English<br>which was released in 2013. This dataset contains about one billion words, and<br>has a vocabulary size of about 800K words. It contains mostly news data. Since<br>sentences in the training set are shuffled, models can ignore the context and<br>focus on sentence level language modeling.</p>
<p>In the original release and subsequent work, people have used the same test set<br>to train models on this dataset as a standard benchmark for language modeling.<br>Recently, we wrote an article (<a href="http://arxiv.org/abs/1602.02410" target="_blank" rel="noopener">http://arxiv.org/abs/1602.02410</a>) describing a<br>model hybrid between character CNN, a large and deep LSTM, and a specific<br>Softmax architecture which allowed us to train the best model on this dataset<br>thus far, almost halving the best perplexity previously obtained by others.</p>
<p><b>Code Release</b></p>
<p>The open-sourced components include:</p>
<ul>
<li>TensorFlow GraphDef proto buffer text file.</li>
<li>TensorFlow pre-trained checkpoint shards.</li>
<li>Code used to evaluate the pre-trained model.</li>
<li>Vocabulary file.</li>
<li>Test set from LM-1B evaluation.</li>
</ul>
<p>The code supports 4 evaluation modes:</p>
<ul>
<li>Given provided dataset, calculate the model’s perplexity.</li>
<li>Given a prefix sentence, predict the next words.</li>
<li>Dump the softmax embedding, character-level CNN word embeddings.</li>
<li>Give a sentence, dump the embedding from the LSTM state.</li>
</ul>
<p><b>Results</b></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>Test Perplexity</th>
<th>Number of Params [billions]</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoid-RNN-2048 [Blackout]</td>
<td>68.3</td>
<td>4.1</td>
</tr>
<tr>
<td>Interpolated KN 5-gram, 1.1B n-grams [chelba2013one]</td>
<td>67.6</td>
<td>1.76</td>
</tr>
<tr>
<td>Sparse Non-Negative Matrix LM [shazeer2015sparse]</td>
<td>52.9</td>
<td>33</td>
</tr>
<tr>
<td>RNN-1024 + MaxEnt 9-gram features [chelba2013one]</td>
<td>51.3</td>
<td>20</td>
</tr>
<tr>
<td>LSTM-512-512</td>
<td>54.1</td>
<td>0.82</td>
</tr>
<tr>
<td>LSTM-1024-512</td>
<td>48.2</td>
<td>0.82</td>
</tr>
<tr>
<td>LSTM-2048-512</td>
<td>43.7</td>
<td>0.83</td>
</tr>
<tr>
<td>LSTM-8192-2048 (No Dropout)</td>
<td>37.9</td>
<td>3.3</td>
</tr>
<tr>
<td>LSTM-8192-2048 (50\% Dropout)</td>
<td>32.2</td>
<td>3.3</td>
</tr>
<tr>
<td>2-Layer LSTM-8192-1024 (BIG LSTM)</td>
<td>30.6</td>
<td>1.8</td>
</tr>
<tr>
<td>(THIS RELEASE) BIG LSTM+CNN Inputs</td>
<td><b>30.0</b></td>
<td><b>1.04</b></td>
</tr>
</tbody>
</table>
</div>
<p><b>How To Run</b></p>
<p>Prerequisites:</p>
<ul>
<li>Install TensorFlow.</li>
<li>Install Bazel.</li>
<li>Download the data files:<ul>
<li>Model GraphDef file:<br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/graph-2016-09-10.pbtxt" target="_blank" rel="noopener">link</a></li>
<li>Model Checkpoint sharded file:<br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-base" target="_blank" rel="noopener">1</a><br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-char-embedding" target="_blank" rel="noopener">2</a><br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-lstm" target="_blank" rel="noopener">3</a><br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax0" target="_blank" rel="noopener">4</a><br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax1" target="_blank" rel="noopener">5</a><br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax2" target="_blank" rel="noopener">6</a><br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax3" target="_blank" rel="noopener">7</a><br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax4" target="_blank" rel="noopener">8</a><br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax5" target="_blank" rel="noopener">9</a><br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax6" target="_blank" rel="noopener">10</a><br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax7" target="_blank" rel="noopener">11</a><br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/all_shards-2016-09-10/ckpt-softmax8" target="_blank" rel="noopener">12</a></li>
<li>Vocabulary file:<br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/vocab-2016-09-10.txt" target="_blank" rel="noopener">link</a></li>
<li>test dataset: link<br><a href="http://download.tensorflow.org/models/LM_LSTM_CNN/test/news.en.heldout-00000-of-00050" target="_blank" rel="noopener">link</a></li>
</ul>
</li>
<li>It is recommended to run on a modern desktop instead of a laptop.</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1. Clone the code to your workspace.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. Download the data to your workspace.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3. Create an empty WORKSPACE file <span class="keyword">in</span> your workspace.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4. Create an empty output directory <span class="keyword">in</span> your workspace.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Example directory structure below:</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls -R</span></span><br><span class="line">.:</span><br><span class="line">data  lm_1b  output  WORKSPACE</span><br><span class="line"></span><br><span class="line">./data:</span><br><span class="line">ckpt-base            ckpt-lstm      ckpt-softmax1  ckpt-softmax3  ckpt-softmax5</span><br><span class="line">ckpt-softmax7  graph-2016-09-10.pbtxt          vocab-2016-09-10.txt</span><br><span class="line">ckpt-char-embedding  ckpt-softmax0  ckpt-softmax2  ckpt-softmax4  ckpt-softmax6</span><br><span class="line">ckpt-softmax8  news.en.heldout-00000-of-00050</span><br><span class="line"></span><br><span class="line">./lm_1b:</span><br><span class="line">BUILD  data_utils.py  lm_1b_eval.py  README.md</span><br><span class="line"></span><br><span class="line">./output:</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Build the codes.</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> bazel build -c opt lm_1b/...</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run sample mode:</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> bazel-bin/lm_1b/lm_1b_eval --mode sample \</span></span><br><span class="line">                             --prefix "I love that I" \</span><br><span class="line">                             --pbtxt data/graph-2016-09-10.pbtxt \</span><br><span class="line">                             --vocab_file data/vocab-2016-09-10.txt  \</span><br><span class="line">                             --ckpt 'data/ckpt-*'</span><br><span class="line">...(omitted some TensorFlow output)</span><br><span class="line">I love</span><br><span class="line">I love that</span><br><span class="line">I love that I</span><br><span class="line">I love that I find</span><br><span class="line">I love that I find that</span><br><span class="line">I love that I find that amazing</span><br><span class="line">...(omitted)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run <span class="built_in">eval</span> mode:</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> bazel-bin/lm_1b/lm_1b_eval --mode <span class="built_in">eval</span> \</span></span><br><span class="line">                             --pbtxt data/graph-2016-09-10.pbtxt \</span><br><span class="line">                             --vocab_file data/vocab-2016-09-10.txt  \</span><br><span class="line">                             --input_data data/news.en.heldout-00000-of-00050 \</span><br><span class="line">                             --ckpt 'data/ckpt-*'</span><br><span class="line">...(omitted some TensorFlow output)</span><br><span class="line">Loaded step 14108582.</span><br><span class="line"><span class="meta">#</span><span class="bash"> perplexity is high initially because words without context are harder to</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> predict.</span></span><br><span class="line">Eval Step: 0, Average Perplexity: 2045.512297.</span><br><span class="line">Eval Step: 1, Average Perplexity: 229.478699.</span><br><span class="line">Eval Step: 2, Average Perplexity: 208.116787.</span><br><span class="line">Eval Step: 3, Average Perplexity: 338.870601.</span><br><span class="line">Eval Step: 4, Average Perplexity: 228.950107.</span><br><span class="line">Eval Step: 5, Average Perplexity: 197.685857.</span><br><span class="line">Eval Step: 6, Average Perplexity: 156.287063.</span><br><span class="line">Eval Step: 7, Average Perplexity: 124.866189.</span><br><span class="line">Eval Step: 8, Average Perplexity: 147.204975.</span><br><span class="line">Eval Step: 9, Average Perplexity: 90.124864.</span><br><span class="line">Eval Step: 10, Average Perplexity: 59.897914.</span><br><span class="line">Eval Step: 11, Average Perplexity: 42.591137.</span><br><span class="line">...(omitted)</span><br><span class="line">Eval Step: 4529, Average Perplexity: 29.243668.</span><br><span class="line">Eval Step: 4530, Average Perplexity: 29.302362.</span><br><span class="line">Eval Step: 4531, Average Perplexity: 29.285674.</span><br><span class="line">...(omitted. At convergence, it should be around 30.)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run dump_emb mode:</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> bazel-bin/lm_1b/lm_1b_eval --mode dump_emb \</span></span><br><span class="line">                             --pbtxt data/graph-2016-09-10.pbtxt \</span><br><span class="line">                             --vocab_file data/vocab-2016-09-10.txt  \</span><br><span class="line">                             --ckpt 'data/ckpt-*' \</span><br><span class="line">                             --save_dir output</span><br><span class="line">...(omitted some TensorFlow output)</span><br><span class="line">Finished softmax weights</span><br><span class="line">Finished word embedding 0/793471</span><br><span class="line">Finished word embedding 1/793471</span><br><span class="line">Finished word embedding 2/793471</span><br><span class="line">...(omitted)</span><br><span class="line"><span class="meta">$</span><span class="bash"> ls output/</span></span><br><span class="line">embeddings_softmax.npy ...</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run dump_lstm_emb mode:</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> bazel-bin/lm_1b/lm_1b_eval --mode dump_lstm_emb \</span></span><br><span class="line">                             --pbtxt data/graph-2016-09-10.pbtxt \</span><br><span class="line">                             --vocab_file data/vocab-2016-09-10.txt \</span><br><span class="line">                             --ckpt 'data/ckpt-*' \</span><br><span class="line">                             --sentence "I love who I am ." \</span><br><span class="line">                             --save_dir output</span><br><span class="line"><span class="meta">$</span><span class="bash"> ls output/</span></span><br><span class="line">lstm_emb_step_0.npy  lstm_emb_step_2.npy  lstm_emb_step_4.npy</span><br><span class="line">lstm_emb_step_6.npy  lstm_emb_step_1.npy  lstm_emb_step_3.npy</span><br><span class="line">lstm_emb_step_5.npy</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
        
      
        <div id="reward-container">
  <div>本站所有文章和源码均免费开放，如您喜欢，可以请我喝杯咖啡</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.jpg" alt="袁宵 微信支付">
        <p>微信支付</p>
      </div>
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="袁宵 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>袁宵</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yuanxiaosc.github.io/2018/11/27/One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling/" title="待读One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling">https://yuanxiaosc.github.io/2018/11/27/One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/Language-Modeling/" rel="tag"># Language Modeling</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2018/11/27/Universal Transformers/" rel="next" title="待读Universal Transformers">
                  <i class="fa fa-chevron-left"></i> 待读Universal Transformers
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2018/11/27/Bidirectional_Encoder_Representations_Transformers/" rel="prev" title="BERT Pre-training of Deep Bidirectional Transformers for Language Understanding">
                  BERT Pre-training of Deep Bidirectional Transformers for Language Understanding <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#One-Billion-Word-Benchmark-for-Measuring-Progress-in-Statistical-Language-Modeling"><span class="nav-number">1.</span> <span class="nav-text">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Language-Model-on-One-Billion-Word-Benchmark"><span class="nav-number">2.</span> <span class="nav-text">Language Model on One Billion Word Benchmark</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.png"
      alt="袁宵">
  <p class="site-author-name" itemprop="name">袁宵</p>
  <div class="site-description" itemprop="description">专注于人工智能领域研究，特别是自然语言处理。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives">
        
          <span class="site-state-item-count">142</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">133</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/yuanxiaoSC" title="GitHub &rarr; https://github.com/yuanxiaoSC" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:wangzichaochaochao@gmail.com" title="E-Mail &rarr; mailto:wangzichaochaochao@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>
  <div class="cc-license motion-element" itemprop="license">
    
  
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
	  

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">袁宵</span>
</div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5d9c4b1ac4deb418" async="async"></script>
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">全站共 399.5k 字</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/pisces.js?v=7.4.1"></script>

<script src="/js/next-boot.js?v=7.4.1"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>








  <script src="/js/local-search.js?v=7.4.1"></script>














  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
