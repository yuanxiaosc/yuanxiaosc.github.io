<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.7.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-32x32.ico?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="baidu-site-verification" content="eYmWT0dEmt">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":true,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="Recurrent Neural Networks Introduction We introduce Recurrent Neural Networks and how they are able to feed in a sequence and predict either a fixed target (categorical/numerical) or another sequence">
<meta name="keywords" content="自然语言处理,深度学习,机器学习,人工智能,论文">
<meta property="og:type" content="article">
<meta property="og:title" content="循环神经网络 Recurrent Neural Networks">
<meta property="og:url" content="https://yuanxiaosc.github.io/2018/11/29/循环神经网络/index.html">
<meta property="og:site_name" content="望江人工智库">
<meta property="og:description" content="Recurrent Neural Networks Introduction We introduce Recurrent Neural Networks and how they are able to feed in a sequence and predict either a fixed target (categorical/numerical) or another sequence">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://yuanxiaosc.github.io/2018/11/29/循环神经网络/output_19_0.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2018/11/29/循环神经网络/output_19_1.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2018/11/29/循环神经网络/output_25_0.png">
<meta property="og:updated_time" content="2018-11-30T01:32:06.383Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="循环神经网络 Recurrent Neural Networks">
<meta name="twitter:description" content="Recurrent Neural Networks Introduction We introduce Recurrent Neural Networks and how they are able to feed in a sequence and predict either a fixed target (categorical/numerical) or another sequence">
<meta name="twitter:image" content="https://yuanxiaosc.github.io/2018/11/29/循环神经网络/output_19_0.png">
  <link rel="canonical" href="https://yuanxiaosc.github.io/2018/11/29/循环神经网络/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>循环神经网络 Recurrent Neural Networks | 望江人工智库</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?359fbde2215e8ede98cdd58478ab2c53";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">望江人工智库</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">TF-KMP</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="fa fa-search fa-fw"></i>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/yuanxiaosc" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yuanxiaosc.github.io/2018/11/29/循环神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="袁宵">
      <meta itemprop="description" content="专注于机器学习前沿论文（技术）研究和应用，欢迎邮件交流。">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="望江人工智库">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">循环神经网络 Recurrent Neural Networks

          
        </h2>

        <div class="post-meta">
		  	  
			  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
			   

              
                
              

              <time title="创建时间：2018-11-29 19:30:15" itemprop="dateCreated datePublished" datetime="2018-11-29T19:30:15+08:00">2018-11-29</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2018-11-30 09:32:06" itemprop="dateModified" datetime="2018-11-30T09:32:06+08:00">2018-11-30</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/循环神经网络/" itemprop="url" rel="index"><span itemprop="name">循环神经网络</span></a></span>

                
                
              
            </span>
          

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/11/29/循环神经网络/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2018/11/29/循环神经网络/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a><a href="https://github.com/nfmcclure/tensorflow_cookbook/tree/master/09_Recurrent_Neural_Networks" target="_blank" rel="noopener">Recurrent Neural Networks</a></h2><ol>
<li><a href="01_Introduction#introduction-to-rnns-in-tensorflow">Introduction</a><ul>
<li>We introduce Recurrent Neural Networks and how they are able to feed in a sequence and predict either a fixed target (categorical/numerical) or another sequence (sequence to sequence).</li>
</ul>
</li>
<li><a href="02_Implementing_RNN_for_Spam_Prediction#implementing-an-rnn-for-spam-prediction">Implementing an RNN Model for Spam Prediction</a><ul>
<li>We create an RNN model to improve on our spam/ham SMS text predictions.</li>
</ul>
</li>
<li><a href="03_Implementing_LSTM#implementing-an-lstm-model">Implementing an LSTM Model for Text Generation</a><ul>
<li>We show how to implement a LSTM (Long Short Term Memory) RNN for Shakespeare language generation. (Word level vocabulary)</li>
</ul>
</li>
<li><a href="04_Stacking_Multiple_LSTM_Layers#stacking-multiple-lstm-layers">Stacking Multiple LSTM Layers</a><ul>
<li>We stack multiple LSTM layers to improve on our Shakespeare language generation. (Character level vocabulary)</li>
</ul>
</li>
<li><a href="05_Creating_A_Sequence_To_Sequence_Model#creating-a-sequence-to-sequence-model-with-tensorflow-seq2seq">Creating a Sequence to Sequence Translation Model (Seq2Seq)</a><ul>
<li>We show how to use TensorFlow’s sequence-to-sequence models to train an English-German translation model.</li>
</ul>
</li>
<li><a href="06_Training_A_Siamese_Similarity_Measure#training-a-siamese-similarity-measure-rnns">Training a Siamese Similarity Measure</a><ul>
<li>Here, we implement a Siamese RNN to predict the similarity of addresses and use it for record matching.  Using RNNs for record matching is very versatile, as we do not have a fixed set of target categories and can use the trained model to predict similarities across new addresses.</li>
</ul>
</li>
</ol><a id="more"></a>
<h2 id="Implementing-an-RNN-in-TensorFlow"><a href="#Implementing-an-RNN-in-TensorFlow" class="headerlink" title="Implementing an RNN in TensorFlow"></a><a href="https://github.com/nfmcclure/tensorflow_cookbook/tree/master/09_Recurrent_Neural_Networks/02_Implementing_RNN_for_Spam_Prediction" target="_blank" rel="noopener">Implementing an RNN in TensorFlow</a></h2><p>This script implements an RNN in TensorFlow to predict spam/ham from texts.</p>
<p>We start by loading the necessary libraries and initializing a computation graph in TensorFlow.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line">ops.reset_default_graph()</span><br><span class="line"><span class="comment"># Start a graph</span></span><br><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure>
<p>Next we set the parameters for the RNN model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set RNN parameters</span></span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line">batch_size = <span class="number">250</span></span><br><span class="line">max_sequence_length = <span class="number">25</span></span><br><span class="line">rnn_size = <span class="number">10</span></span><br><span class="line">embedding_size = <span class="number">50</span></span><br><span class="line">min_word_frequency = <span class="number">10</span></span><br><span class="line">learning_rate = <span class="number">0.0005</span></span><br><span class="line">dropout_keep_prob = tf.placeholder(tf.float32)</span><br></pre></td></tr></table></figure>
<p>We download and save the data next.  First we check if we have saved it before and load it locally, if not, we load it from the internet (UCI machine learning data repository).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Download or open data</span></span><br><span class="line">data_dir = <span class="string">'temp'</span></span><br><span class="line">data_file = <span class="string">'text_data.txt'</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(data_dir):</span><br><span class="line">    os.makedirs(data_dir)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(os.path.join(data_dir, data_file)):</span><br><span class="line">    zip_url = <span class="string">'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'</span></span><br><span class="line">    r = requests.get(zip_url)</span><br><span class="line">    z = ZipFile(io.BytesIO(r.content))</span><br><span class="line">    file = z.read(<span class="string">'SMSSpamCollection'</span>)</span><br><span class="line">    <span class="comment"># Format Data</span></span><br><span class="line">    text_data = file.decode()</span><br><span class="line">    text_data = text_data.encode(<span class="string">'ascii'</span>, errors=<span class="string">'ignore'</span>)</span><br><span class="line">    text_data = text_data.decode().split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save data to text file</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(data_dir, data_file), <span class="string">'w'</span>) <span class="keyword">as</span> file_conn:</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> text_data:</span><br><span class="line">            file_conn.write(<span class="string">"&#123;&#125;\n"</span>.format(text))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Open data from text file</span></span><br><span class="line">    text_data = []</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(data_dir, data_file), <span class="string">'r'</span>) <span class="keyword">as</span> file_conn:</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> file_conn:</span><br><span class="line">            text_data.append(row)</span><br><span class="line">    text_data = text_data[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">text_data = [x.split(<span class="string">'\t'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> text_data <span class="keyword">if</span> len(x) &gt;= <span class="number">1</span>]</span><br><span class="line">[text_data_target, text_data_train] = [list(x) <span class="keyword">for</span> x <span class="keyword">in</span> zip(*text_data)]</span><br></pre></td></tr></table></figure>
<p>Next, we process the texts and turn them into numeric representations (words —&gt; indices).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a text cleaning function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span><span class="params">(text_string)</span>:</span></span><br><span class="line">    text_string = re.sub(<span class="string">r'([^\s\w]|_|[0-9])+'</span>, <span class="string">''</span>, text_string)</span><br><span class="line">    text_string = <span class="string">" "</span>.join(text_string.split())</span><br><span class="line">    text_string = text_string.lower()</span><br><span class="line">    <span class="keyword">return</span> text_string</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Clean texts</span></span><br><span class="line">text_data_train = [clean_text(x) <span class="keyword">for</span> x <span class="keyword">in</span> text_data_train]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change texts into numeric vectors</span></span><br><span class="line">vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,</span><br><span class="line">                                                                     min_frequency=min_word_frequency)</span><br><span class="line">text_processed = np.array(list(vocab_processor.fit_transform(text_data_train)))</span><br></pre></td></tr></table></figure>
<p>Now we shuffle and split the texts into train/tests (80% training, 20% testing).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Shuffle and split data</span></span><br><span class="line">text_processed = np.array(text_processed)</span><br><span class="line">text_data_target = np.array([<span class="number">1</span> <span class="keyword">if</span> x == <span class="string">'ham'</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> text_data_target])</span><br><span class="line">shuffled_ix = np.random.permutation(np.arange(len(text_data_target)))</span><br><span class="line">x_shuffled = text_processed[shuffled_ix]</span><br><span class="line">y_shuffled = text_data_target[shuffled_ix]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split train/test set</span></span><br><span class="line">ix_cutoff = int(len(y_shuffled)*<span class="number">0.80</span>)</span><br><span class="line">x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]</span><br><span class="line">y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]</span><br><span class="line">vocab_size = len(vocab_processor.vocabulary_)</span><br><span class="line">print(<span class="string">"Vocabulary Size: &#123;:d&#125;"</span>.format(vocab_size))</span><br><span class="line">print(<span class="string">"80-20 Train Test split: &#123;:d&#125; -- &#123;:d&#125;"</span>.format(len(y_train), len(y_test)))</span><br></pre></td></tr></table></figure>
<pre><code>Vocabulary Size: 933
80-20 Train Test split: 4459 -- 1115
</code></pre><p>Here we can define our RNN model.  We create the placeholders for the data, word embedding matrices (and embedding lookups), and define the rest of the model.</p>
<p>The rest of the RNN model will create a dynamic RNN cell (regular RNN type), which will vary the number of RNNs needed for variable input length (different amount of words for input texts), and then output into a fully connected logistic layer to predict spam or ham as output.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create placeholders</span></span><br><span class="line">x_data = tf.placeholder(tf.int32, [<span class="keyword">None</span>, max_sequence_length])</span><br><span class="line">y_output = tf.placeholder(tf.int32, [<span class="keyword">None</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create embedding</span></span><br><span class="line">embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the RNN cell</span></span><br><span class="line"><span class="comment"># tensorflow change &gt;= 1.0, rnn is put into tensorflow.contrib directory. Prior version not test.</span></span><br><span class="line"><span class="keyword">if</span> tf.__version__[<span class="number">0</span>] &gt;= <span class="string">'1'</span>:</span><br><span class="line">    cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    cell = tf.nn.rnn_cell.BasicRNNCell(num_units=rnn_size)</span><br><span class="line"></span><br><span class="line">output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)</span><br><span class="line">output = tf.nn.dropout(output, dropout_keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get output of RNN sequence</span></span><br><span class="line"><span class="comment">#output = tf.transpose(output, [1, 0, 2])</span></span><br><span class="line"><span class="comment">#last = tf.gather(output, int(output.get_shape()[0]) - 1)</span></span><br><span class="line"></span><br><span class="line">last = output[:,<span class="number">-1</span>,:]</span><br><span class="line"></span><br><span class="line">weight = tf.Variable(tf.truncated_normal([rnn_size, <span class="number">2</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">bias = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">2</span>]))</span><br><span class="line">logits_out = tf.matmul(last, weight) + bias</span><br></pre></td></tr></table></figure>
<p>Next we declare the loss function (softmax cross entropy), an accuracy function, and optimization function (RMSProp).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss function</span></span><br><span class="line">losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output)</span><br><span class="line">loss = tf.reduce_mean(losses)</span><br><span class="line"></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, <span class="number">1</span>), tf.cast(y_output, tf.int64)), tf.float32))</span><br><span class="line"></span><br><span class="line">optimizer = tf.train.RMSPropOptimizer(learning_rate)</span><br><span class="line">train_step = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>You may ignore the warning, as the texts are small and our batch size is only 100.  If you increase the batch size and/or have longer sequences of texts, this model may consume too much memory.</p>
</blockquote>
<p>Next we initialize the variables in the computational graph.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line">train_loss = []</span><br><span class="line">test_loss = []</span><br><span class="line">train_accuracy = []</span><br><span class="line">test_accuracy = []</span><br></pre></td></tr></table></figure>
<p>Now we can start our training!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Shuffle training data</span></span><br><span class="line">    shuffled_ix = np.random.permutation(np.arange(len(x_train)))</span><br><span class="line">    x_train = x_train[shuffled_ix]</span><br><span class="line">    y_train = y_train[shuffled_ix]</span><br><span class="line">    num_batches = int(len(x_train)/batch_size) + <span class="number">1</span></span><br><span class="line">    <span class="comment"># TO DO CALCULATE GENERATIONS ExACTLY</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches):</span><br><span class="line">        <span class="comment"># Select train data</span></span><br><span class="line">        min_ix = i * batch_size</span><br><span class="line">        max_ix = np.min([len(x_train), ((i+<span class="number">1</span>) * batch_size)])</span><br><span class="line">        x_train_batch = x_train[min_ix:max_ix]</span><br><span class="line">        y_train_batch = y_train[min_ix:max_ix]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run train step</span></span><br><span class="line">        train_dict = &#123;x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob:<span class="number">0.5</span>&#125;</span><br><span class="line">        sess.run(train_step, feed_dict=train_dict)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run loss and accuracy for training</span></span><br><span class="line">    temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)</span><br><span class="line">    train_loss.append(temp_train_loss)</span><br><span class="line">    train_accuracy.append(temp_train_acc)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run Eval Step</span></span><br><span class="line">    test_dict = &#123;x_data: x_test, y_output: y_test, dropout_keep_prob:<span class="number">1.0</span>&#125;</span><br><span class="line">    temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)</span><br><span class="line">    test_loss.append(temp_test_loss)</span><br><span class="line">    test_accuracy.append(temp_test_acc)</span><br><span class="line">    print(<span class="string">'Epoch: &#123;&#125;, Test Loss: &#123;:.2&#125;, Test Acc: &#123;:.2&#125;'</span>.format(epoch+<span class="number">1</span>, temp_test_loss, temp_test_acc))</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 1, Test Loss: 0.71, Test Acc: 0.17
Epoch: 2, Test Loss: 0.68, Test Acc: 0.82
...
Epoch: 50, Test Loss: 0.12, Test Acc: 0.96
</code></pre><p>Here is matplotlib code to plot the loss and accuracy over the training generations for both the train and test sets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot loss over time</span></span><br><span class="line">epoch_seq = np.arange(<span class="number">1</span>, epochs+<span class="number">1</span>)</span><br><span class="line">plt.plot(epoch_seq, train_loss, <span class="string">'k--'</span>, label=<span class="string">'Train Set'</span>)</span><br><span class="line">plt.plot(epoch_seq, test_loss, <span class="string">'r-'</span>, label=<span class="string">'Test Set'</span>)</span><br><span class="line">plt.title(<span class="string">'Softmax Loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Softmax Loss'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot accuracy over time</span></span><br><span class="line">plt.plot(epoch_seq, train_accuracy, <span class="string">'k--'</span>, label=<span class="string">'Train Set'</span>)</span><br><span class="line">plt.plot(epoch_seq, test_accuracy, <span class="string">'r-'</span>, label=<span class="string">'Test Set'</span>)</span><br><span class="line">plt.title(<span class="string">'Test Accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/11/29/循环神经网络/output_19_0.png" alt="png"></p>
<p><img src="/2018/11/29/循环神经网络/output_19_1.png" alt="png"></p>
<h3 id="Evaluating-New-Texts"><a href="#Evaluating-New-Texts" class="headerlink" title="Evaluating New Texts"></a>Evaluating New Texts</h3><p>Here, we show how to use our trained model to evaluate new texts (which may or may not be spam/ham)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sample_texts = [<span class="string">'Hi, please respond 1111 asap to claim your change to win now!'</span>,</span><br><span class="line">                <span class="string">'Hey what are you doing for dinner tonight?'</span>,</span><br><span class="line">                <span class="string">'New offer, show this text for 50% off of our inagural sale!'</span>,</span><br><span class="line">                <span class="string">'Can you take the dog to the vet tomorrow?'</span>,</span><br><span class="line">                <span class="string">'Congratulations! You have been randomly selected to receive account credit!'</span>]</span><br></pre></td></tr></table></figure>
<p>Now we clean our sample texts.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clean_texts = [clean_text(text) <span class="keyword">for</span> text <span class="keyword">in</span> sample_texts]</span><br><span class="line">print(clean_texts)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;hi please respond asap to claim your change to win now&#39;, &#39;hey what are you doing for dinner tonight&#39;, &#39;new offer show this text for off of our inagural sale&#39;, &#39;can you take the dog to the vet tomorrow&#39;, &#39;congratulations you have been randomly selected to receive account credit&#39;]
</code></pre><p>Next, we transform each text as a sequence of words into a sequence of vocabulary indices.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">processed_texts = np.array(list(vocab_processor.transform(clean_texts)))</span><br><span class="line">print(processed_texts)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 93  99   0   0   1 114  13 524   1 178  21   0   0   0   0   0   0   0
    0   0   0   0   0   0   0]
 [121  52  20   3 151  12 332 208   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0]
 [ 92 376 483  39  69  12 203  15  86   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0]
 [ 28   3 104   5   0   1   5   0 143   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0]
 [701   3  17  98   0 420   1 318 301 738   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0]]
</code></pre><p>Now we can run each of the texts through our model and get the output logits.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remember to wrap the resulting logits in a softmax to get probabilities</span></span><br><span class="line">eval_feed_dict = &#123;x_data: processed_texts, dropout_keep_prob: <span class="number">1.0</span>&#125;</span><br><span class="line">model_results = sess.run(tf.nn.softmax(logits_out), feed_dict=eval_feed_dict)</span><br><span class="line"></span><br><span class="line">print(model_results)</span><br></pre></td></tr></table></figure>
<pre><code>[[0.86792374 0.13207628]
 [0.00838861 0.9916114 ]
 [0.00871871 0.99128133]
 [0.00838833 0.99161166]
 [0.6345383  0.36546162]]
</code></pre><p>Now print results</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">categories = [<span class="string">'spam'</span>, <span class="string">'ham'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ix, result <span class="keyword">in</span> enumerate(model_results):</span><br><span class="line">    prediction = categories[np.argmax(result)]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Text: &#123;&#125;, \nPrediction: &#123;&#125;\n'</span>.format(sample_texts[ix], prediction))</span><br></pre></td></tr></table></figure>
<pre><code>Text: Hi, please respond 1111 asap to claim your change to win now!,
Prediction: spam

Text: Hey what are you doing for dinner tonight?,
Prediction: ham

Text: New offer, show this text for 50% off of our inagural sale!,
Prediction: ham

Text: Can you take the dog to the vet tomorrow?,
Prediction: ham

Text: Congratulations! You have been randomly selected to receive account credit!,
Prediction: spam
</code></pre><h2 id="Implementing-an-LSTM-RNN-Model"><a href="#Implementing-an-LSTM-RNN-Model" class="headerlink" title="Implementing an LSTM RNN Model"></a><a href="https://github.com/nfmcclure/tensorflow_cookbook/tree/master/09_Recurrent_Neural_Networks/03_Implementing_LSTM" target="_blank" rel="noopener">Implementing an LSTM RNN Model</a></h2><p>Here we implement an LSTM model on all a data set of Shakespeare works.</p>
<p>We start by loading the necessary libraries and resetting the default computational graph.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line">ops.reset_default_graph()</span><br></pre></td></tr></table></figure>
<p>We start a computational graph session.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure>
<p>Next, it is important to set the algorithm and data processing parameters.</p>
<hr>
<p>Parameter  :  Descriptions</p>
<ul>
<li>min_word_freq: Only attempt to model words that appear at least 5 times.</li>
<li>rnn_size: size of our RNN (equal to the embedding size)</li>
<li>epochs: Number of epochs to cycle through the data</li>
<li>batch_size: How many examples to train on at once</li>
<li>learning_rate: The learning rate or the convergence paramter</li>
<li>training_seq_len: The length of the surrounding word group (e.g. 10 = 5 on each side)</li>
<li>embedding_size: Must be equal to the rnn_size</li>
<li>save_every: How often to save the model</li>
<li>eval_every: How often to evaluate the model</li>
<li>prime_texts: List of test sentences</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set RNN Parameters</span></span><br><span class="line">min_word_freq = <span class="number">5</span> <span class="comment"># Trim the less frequent words off</span></span><br><span class="line">rnn_size = <span class="number">128</span> <span class="comment"># RNN Model size</span></span><br><span class="line">embedding_size = <span class="number">100</span> <span class="comment"># Word embedding size</span></span><br><span class="line">epochs = <span class="number">10</span> <span class="comment"># Number of epochs to cycle through data</span></span><br><span class="line">batch_size = <span class="number">100</span> <span class="comment"># Train on this many examples at once</span></span><br><span class="line">learning_rate = <span class="number">0.001</span> <span class="comment"># Learning rate</span></span><br><span class="line">training_seq_len = <span class="number">50</span> <span class="comment"># how long of a word group to consider</span></span><br><span class="line"><span class="comment">#embedding_size = rnn_size</span></span><br><span class="line">save_every = <span class="number">500</span> <span class="comment"># How often to save model checkpoints</span></span><br><span class="line">eval_every = <span class="number">50</span> <span class="comment"># How often to evaluate the test sentences</span></span><br><span class="line">prime_texts = [<span class="string">'thou art more'</span>, <span class="string">'to be or not to'</span>, <span class="string">'wherefore art thou'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download/store Shakespeare data</span></span><br><span class="line">data_dir = <span class="string">'temp'</span></span><br><span class="line">data_file = <span class="string">'shakespeare.txt'</span></span><br><span class="line">model_path = <span class="string">'shakespeare_model'</span></span><br><span class="line">full_model_dir = os.path.join(data_dir, model_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Declare punctuation to remove, everything except hyphens and apostrophes</span></span><br><span class="line">punctuation = string.punctuation</span><br><span class="line">punctuation = <span class="string">''</span>.join([x <span class="keyword">for</span> x <span class="keyword">in</span> punctuation <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'-'</span>, <span class="string">"'"</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make Model Directory</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(full_model_dir):</span><br><span class="line">    os.makedirs(full_model_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make data directory</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(data_dir):</span><br><span class="line">    os.makedirs(data_dir)</span><br></pre></td></tr></table></figure>
<p>Download the data if we don’t have it saved already.  The data comes from the <a href="http://www.gutenberg.org]" target="_blank" rel="noopener">Gutenberg Project</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Loading Shakespeare Data'</span>)</span><br><span class="line"><span class="comment"># Check if file is downloaded.</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(os.path.join(data_dir, data_file)):</span><br><span class="line">    print(<span class="string">'Not found, downloading Shakespeare texts from www.gutenberg.org'</span>)</span><br><span class="line">    shakespeare_url = <span class="string">'http://www.gutenberg.org/cache/epub/100/pg100.txt'</span></span><br><span class="line">    <span class="comment"># Get Shakespeare text</span></span><br><span class="line">    response = requests.get(shakespeare_url)</span><br><span class="line">    shakespeare_file = response.content</span><br><span class="line">    <span class="comment"># Decode binary into string</span></span><br><span class="line">    s_text = shakespeare_file.decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment"># Drop first few descriptive paragraphs.</span></span><br><span class="line">    s_text = s_text[<span class="number">7675</span>:]</span><br><span class="line">    <span class="comment"># Remove newlines</span></span><br><span class="line">    s_text = s_text.replace(<span class="string">'\r\n'</span>, <span class="string">''</span>)</span><br><span class="line">    s_text = s_text.replace(<span class="string">'\n'</span>, <span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Write to file</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(data_dir, data_file), <span class="string">'w'</span>) <span class="keyword">as</span> out_conn:</span><br><span class="line">        out_conn.write(s_text)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># If file has been saved, load from that file</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(data_dir, data_file), <span class="string">'r'</span>) <span class="keyword">as</span> file_conn:</span><br><span class="line">        s_text = file_conn.read().replace(<span class="string">'\n'</span>, <span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clean text</span></span><br><span class="line">print(<span class="string">'Cleaning Text'</span>)</span><br><span class="line">s_text = re.sub(<span class="string">r'[&#123;&#125;]'</span>.format(punctuation), <span class="string">' '</span>, s_text)</span><br><span class="line">s_text = re.sub(<span class="string">'\s+'</span>, <span class="string">' '</span>, s_text ).strip().lower()</span><br><span class="line">print(<span class="string">'Done loading/cleaning.'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Loading Shakespeare Data
Cleaning Text
Done loading/cleaning.
</code></pre><p>Define a function to build a word processing dictionary (word -&gt; ix)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build word vocabulary function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(text, min_word_freq)</span>:</span></span><br><span class="line">    word_counts = collections.Counter(text.split(<span class="string">' '</span>))</span><br><span class="line">    <span class="comment"># limit word counts to those more frequent than cutoff</span></span><br><span class="line">    word_counts = &#123;key:val <span class="keyword">for</span> key, val <span class="keyword">in</span> word_counts.items() <span class="keyword">if</span> val&gt;min_word_freq&#125;</span><br><span class="line">    <span class="comment"># Create vocab --&gt; index mapping</span></span><br><span class="line">    words = word_counts.keys()</span><br><span class="line">    vocab_to_ix_dict = &#123;key:(ix+<span class="number">1</span>) <span class="keyword">for</span> ix, key <span class="keyword">in</span> enumerate(words)&#125;</span><br><span class="line">    <span class="comment"># Add unknown key --&gt; 0 index</span></span><br><span class="line">    vocab_to_ix_dict[<span class="string">'unknown'</span>]=<span class="number">0</span></span><br><span class="line">    <span class="comment"># Create index --&gt; vocab mapping</span></span><br><span class="line">    ix_to_vocab_dict = &#123;val:key <span class="keyword">for</span> key,val <span class="keyword">in</span> vocab_to_ix_dict.items()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>(ix_to_vocab_dict, vocab_to_ix_dict)</span><br></pre></td></tr></table></figure>
<p>Now we can build the index-vocabulary from the Shakespeare data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build Shakespeare vocabulary</span></span><br><span class="line">print(<span class="string">'Building Shakespeare Vocab'</span>)</span><br><span class="line">ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)</span><br><span class="line">vocab_size = len(ix2vocab) + <span class="number">1</span></span><br><span class="line">print(<span class="string">'Vocabulary Length = &#123;&#125;'</span>.format(vocab_size))</span><br><span class="line"><span class="comment"># Sanity Check</span></span><br><span class="line"><span class="keyword">assert</span>(len(ix2vocab) == len(vocab2ix))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert text to word vectors</span></span><br><span class="line">s_text_words = s_text.split(<span class="string">' '</span>)</span><br><span class="line">s_text_ix = []</span><br><span class="line"><span class="keyword">for</span> ix, x <span class="keyword">in</span> enumerate(s_text_words):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        s_text_ix.append(vocab2ix[x])</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        s_text_ix.append(<span class="number">0</span>)</span><br><span class="line">s_text_ix = np.array(s_text_ix)</span><br></pre></td></tr></table></figure>
<pre><code>Building Shakespeare Vocab
Vocabulary Length = 8009
</code></pre><p>We define the LSTM model.  The methods of interest are the <code>__init__()</code> method, which defines all the model variables and operations, and the <code>sample()</code> method which takes in a sample word and loops through to generate text.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define LSTM RNN Model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM_Model</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedding_size, rnn_size, batch_size, learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                 training_seq_len, vocab_size, infer_sample=False)</span>:</span></span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.rnn_size = rnn_size</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.infer_sample = infer_sample</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> infer_sample:</span><br><span class="line">            self.batch_size = <span class="number">1</span></span><br><span class="line">            self.training_seq_len = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.batch_size = batch_size</span><br><span class="line">            self.training_seq_len = training_seq_len</span><br><span class="line"></span><br><span class="line">        self.lstm_cell = tf.nn.rnn_cell.LSTMCell(self.rnn_size)</span><br><span class="line">        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)</span><br><span class="line"></span><br><span class="line">        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])</span><br><span class="line">        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'lstm_vars'</span>):</span><br><span class="line">            <span class="comment"># Softmax Output Weights</span></span><br><span class="line">            W = tf.get_variable(<span class="string">'W'</span>, [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())</span><br><span class="line">            b = tf.get_variable(<span class="string">'b'</span>, [self.vocab_size], tf.float32, tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Define Embedding</span></span><br><span class="line">            embedding_mat = tf.get_variable(<span class="string">'embedding_mat'</span>, [self.vocab_size, self.embedding_size],</span><br><span class="line">                                            tf.float32, tf.random_normal_initializer())</span><br><span class="line"></span><br><span class="line">            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)</span><br><span class="line">            rnn_inputs = tf.split(axis=<span class="number">1</span>, num_or_size_splits=self.training_seq_len, value=embedding_output)</span><br><span class="line">            rnn_inputs_trimmed = [tf.squeeze(x, [<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> rnn_inputs]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If we are inferring (generating text), we add a 'loop' function</span></span><br><span class="line">        <span class="comment"># Define how to get the i+1 th input from the i th output</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">inferred_loop</span><span class="params">(prev, count)</span>:</span></span><br><span class="line">            <span class="comment"># Apply hidden layer</span></span><br><span class="line">            prev_transformed = tf.matmul(prev, W) + b</span><br><span class="line">            <span class="comment"># Get the index of the output (also don't run the gradient)</span></span><br><span class="line">            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, <span class="number">1</span>))</span><br><span class="line">            <span class="comment"># Get embedded vector</span></span><br><span class="line">            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)</span><br><span class="line">            <span class="keyword">return</span>(output)</span><br><span class="line"></span><br><span class="line">        decoder = tf.contrib.legacy_seq2seq.rnn_decoder</span><br><span class="line">        outputs, last_state = decoder(rnn_inputs_trimmed,</span><br><span class="line">                                      self.initial_state,</span><br><span class="line">                                      self.lstm_cell,</span><br><span class="line">                                      loop_function=inferred_loop <span class="keyword">if</span> infer_sample <span class="keyword">else</span> <span class="keyword">None</span>)</span><br><span class="line">        <span class="comment"># Non inferred outputs</span></span><br><span class="line">        output = tf.reshape(tf.concat(axis=<span class="number">1</span>, values=outputs), [<span class="number">-1</span>, self.rnn_size])</span><br><span class="line">        <span class="comment"># Logits and output</span></span><br><span class="line">        self.logit_output = tf.matmul(output, W) + b</span><br><span class="line">        self.model_output = tf.nn.softmax(self.logit_output)</span><br><span class="line"></span><br><span class="line">        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example</span><br><span class="line">        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [<span class="number">-1</span>])],</span><br><span class="line">                [tf.ones([self.batch_size * self.training_seq_len])],</span><br><span class="line">                self.vocab_size)</span><br><span class="line">        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)</span><br><span class="line">        self.final_state = last_state</span><br><span class="line">        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), <span class="number">4.5</span>)</span><br><span class="line">        optimizer = tf.train.AdamOptimizer(self.learning_rate)</span><br><span class="line">        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, sess, words=ix2vocab, vocab=vocab2ix, num=<span class="number">10</span>, prime_text=<span class="string">'thou art'</span>)</span>:</span></span><br><span class="line">        state = sess.run(self.lstm_cell.zero_state(<span class="number">1</span>, tf.float32))</span><br><span class="line">        word_list = prime_text.split()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> word_list[:<span class="number">-1</span>]:</span><br><span class="line">            x = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            x[<span class="number">0</span>, <span class="number">0</span>] = vocab[word]</span><br><span class="line">            feed_dict = &#123;self.x_data: x, self.initial_state:state&#125;</span><br><span class="line">            [state] = sess.run([self.final_state], feed_dict=feed_dict)</span><br><span class="line"></span><br><span class="line">        out_sentence = prime_text</span><br><span class="line">        word = word_list[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(num):</span><br><span class="line">            x = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            x[<span class="number">0</span>, <span class="number">0</span>] = vocab[word]</span><br><span class="line">            feed_dict = &#123;self.x_data: x, self.initial_state:state&#125;</span><br><span class="line">            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)</span><br><span class="line">            sample = np.argmax(model_output[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">if</span> sample == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            word = words[sample]</span><br><span class="line">            out_sentence = out_sentence + <span class="string">' '</span> + word</span><br><span class="line">        <span class="keyword">return</span>(out_sentence)</span><br></pre></td></tr></table></figure>
<p>In order to use the same model (with the same trained variables), we need to share the variable scope between the trained model and the test model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define LSTM Model</span></span><br><span class="line">lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,</span><br><span class="line">                        training_seq_len, vocab_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tell TensorFlow we are reusing the scope for the testing</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(tf.get_variable_scope(), reuse=<span class="keyword">True</span>):</span><br><span class="line">    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,</span><br><span class="line">                                 training_seq_len, vocab_size, infer_sample=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>We need to save the model, so we create a model saving operation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create model saver</span></span><br><span class="line">saver = tf.train.Saver(tf.global_variables())</span><br></pre></td></tr></table></figure>
<p>Let’s calculate how many batches are needed for each epoch and split up the data accordingly.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create batches for each epoch</span></span><br><span class="line">num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + <span class="number">1</span></span><br><span class="line"><span class="comment"># Split up text indices into subarrays, of equal size</span></span><br><span class="line">batches = np.array_split(s_text_ix, num_batches)</span><br><span class="line"><span class="comment"># Reshape each split into [batch_size, training_seq_len]</span></span><br><span class="line">batches = [np.resize(x, [batch_size, training_seq_len]) <span class="keyword">for</span> x <span class="keyword">in</span> batches]</span><br></pre></td></tr></table></figure>
<p>Initialize all the variables</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize all variables</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>
<p>Training the model!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train model</span></span><br><span class="line">train_loss = []</span><br><span class="line">iteration_count = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    <span class="comment"># Shuffle word indices</span></span><br><span class="line">    random.shuffle(batches)</span><br><span class="line">    <span class="comment"># Create targets from shuffled batches</span></span><br><span class="line">    targets = [np.roll(x, <span class="number">-1</span>, axis=<span class="number">1</span>) <span class="keyword">for</span> x <span class="keyword">in</span> batches]</span><br><span class="line">    <span class="comment"># Run a through one epoch</span></span><br><span class="line">    print(<span class="string">'Starting Epoch #&#123;&#125; of &#123;&#125;.'</span>.format(epoch+<span class="number">1</span>, epochs))</span><br><span class="line">    <span class="comment"># Reset initial LSTM state every epoch</span></span><br><span class="line">    state = sess.run(lstm_model.initial_state)</span><br><span class="line">    <span class="keyword">for</span> ix, batch <span class="keyword">in</span> enumerate(batches):</span><br><span class="line">        training_dict = &#123;lstm_model.x_data: batch, lstm_model.y_output: targets[ix]&#125;</span><br><span class="line">        c, h = lstm_model.initial_state</span><br><span class="line">        training_dict[c] = state.c</span><br><span class="line">        training_dict[h] = state.h</span><br><span class="line"></span><br><span class="line">        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],</span><br><span class="line">                                       feed_dict=training_dict)</span><br><span class="line">        train_loss.append(temp_loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print status every 10 gens</span></span><br><span class="line">        <span class="keyword">if</span> iteration_count % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            summary_nums = (iteration_count, epoch+<span class="number">1</span>, ix+<span class="number">1</span>, num_batches+<span class="number">1</span>, temp_loss)</span><br><span class="line">            print(<span class="string">'Iteration: &#123;&#125;, Epoch: &#123;&#125;, Batch: &#123;&#125; out of &#123;&#125;, Loss: &#123;:.2f&#125;'</span>.format(*summary_nums))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save the model and the vocab</span></span><br><span class="line">        <span class="keyword">if</span> iteration_count % save_every == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># Save model</span></span><br><span class="line">            model_file_name = os.path.join(full_model_dir, <span class="string">'model'</span>)</span><br><span class="line">            saver.save(sess, model_file_name, global_step = iteration_count)</span><br><span class="line">            print(<span class="string">'Model Saved To: &#123;&#125;'</span>.format(model_file_name))</span><br><span class="line">            <span class="comment"># Save vocabulary</span></span><br><span class="line">            dictionary_file = os.path.join(full_model_dir, <span class="string">'vocab.pkl'</span>)</span><br><span class="line">            <span class="keyword">with</span> open(dictionary_file, <span class="string">'wb'</span>) <span class="keyword">as</span> dict_file_conn:</span><br><span class="line">                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> iteration_count % eval_every == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> sample <span class="keyword">in</span> prime_texts:</span><br><span class="line">                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=<span class="number">10</span>, prime_text=sample))</span><br><span class="line"></span><br><span class="line">        iteration_count += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<pre><code>Starting Epoch #1 of 10.
Iteration: 10, Epoch: 1, Batch: 10 out of 182, Loss: 9.73

thou art more curtain show&#39;rs to the
to be or not to the
wherefore art thou art needs to the
...
Iteration: 1800, Epoch: 10, Batch: 171 out of 182, Loss: 5.71
thou art more than a
to be or not to be
wherefore art thou dost wedded not make me a
Iteration: 1810, Epoch: 10, Batch: 181 out of 182, Loss: 5.56
</code></pre><p>Here is a plot of the training loss across the iterations.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot loss over time</span></span><br><span class="line">plt.plot(train_loss, <span class="string">'k-'</span>)</span><br><span class="line">plt.title(<span class="string">'Sequence to Sequence Loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Iterations'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/11/29/循环神经网络/output_25_0.png" alt="png"></p>

    </div>

    
    
    
        
      
        <div id="reward-container">
  <div>本站所有文章和源码均免费开放，如您喜欢，可以请我喝杯咖啡</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.jpg" alt="袁宵 微信支付">
        <p>微信支付</p>
      </div>
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="袁宵 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>袁宵</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yuanxiaosc.github.io/2018/11/29/循环神经网络/" title="循环神经网络 Recurrent Neural Networks">https://yuanxiaosc.github.io/2018/11/29/循环神经网络/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2018/11/27/中文序列标注/" rel="next" title="中文序列标注">
                  <i class="fa fa-chevron-left"></i> 中文序列标注
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2018/12/02/动手学深度学习Gluon/" rel="prev" title="动手学深度学习Gluon">
                  动手学深度学习Gluon <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Recurrent-Neural-Networks"><span class="nav-number">1.</span> <span class="nav-text">Recurrent Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Implementing-an-RNN-in-TensorFlow"><span class="nav-number">2.</span> <span class="nav-text">Implementing an RNN in TensorFlow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluating-New-Texts"><span class="nav-number">2.1.</span> <span class="nav-text">Evaluating New Texts</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Implementing-an-LSTM-RNN-Model"><span class="nav-number">3.</span> <span class="nav-text">Implementing an LSTM RNN Model</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.png"
      alt="袁宵">
  <p class="site-author-name" itemprop="name">袁宵</p>
  <div class="site-description" itemprop="description">专注于机器学习前沿论文（技术）研究和应用，欢迎邮件交流。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives">
        
          <span class="site-state-item-count">143</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">127</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/yuanxiaoSC" title="GitHub &rarr; https://github.com/yuanxiaoSC" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:wangzichaochaochao@gmail.com" title="E-Mail &rarr; mailto:wangzichaochaochao@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>
  <div class="cc-license motion-element" itemprop="license">
    
  
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
	  

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">袁宵</span>
</div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5d9c4b1ac4deb418" async="async"></script>
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">全站共 381.7k 字</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/pisces.js?v=7.4.1"></script>

<script src="/js/next-boot.js?v=7.4.1"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>








  <script src="/js/local-search.js?v=7.4.1"></script>





  <script src="//code.tidio.co/ohblyq9gicnjwqem8o1hfoymk3calgui.js"></script>









  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'aTXvwHFSoz68yg6g3k5JzN7B-MdYXbMMI',
    appKey: 'Wkf7bKVEfcQ0sW4V1l144HLY',
    placeholder: '写下你的想法',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn',
    path: location.pathname,
    recordIP: false,
    serverURLs: ''
  });
}, window.Valine);
</script>

</body>
</html>
