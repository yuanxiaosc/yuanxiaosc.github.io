<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-32x32.ico?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="baidu-site-verification" content="eYmWT0dEmt">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":true,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="对抗训练基础知识对抗样本定义对抗样本是使得机器学习的算法产生误判的样本，如上图所示，原有的模型以57.7%的置信度判定图片为熊猫，但添加微小的扰动后，模型以99.3%的置信度认为扰动后的图片是长臂猿。产生对抗样本的方法">
<meta name="keywords" content="论文,对抗训练">
<meta property="og:type" content="article">
<meta property="og:title" content="对抗训练（Adversarial Training）">
<meta property="og:url" content="https://yuanxiaosc.github.io/2019/06/05/对抗训练/index.html">
<meta property="og:site_name" content="望江人工智库">
<meta property="og:description" content="对抗训练基础知识对抗样本定义对抗样本是使得机器学习的算法产生误判的样本，如上图所示，原有的模型以57.7%的置信度判定图片为熊猫，但添加微小的扰动后，模型以99.3%的置信度认为扰动后的图片是长臂猿。产生对抗样本的方法">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/06/05/对抗训练/对抗训练例子.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/06/05/对抗训练/2018_Gao_Figure_2.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/06/05/对抗训练/MNIST_Adversarial_Training_1.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/06/05/对抗训练/MNIST_Adversarial_Training_2.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/06/05/对抗训练/扰动样本在NLP领域的应用.png">
<meta property="og:updated_time" content="2019-06-10T06:09:22.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="对抗训练（Adversarial Training）">
<meta name="twitter:description" content="对抗训练基础知识对抗样本定义对抗样本是使得机器学习的算法产生误判的样本，如上图所示，原有的模型以57.7%的置信度判定图片为熊猫，但添加微小的扰动后，模型以99.3%的置信度认为扰动后的图片是长臂猿。产生对抗样本的方法">
<meta name="twitter:image" content="https://yuanxiaosc.github.io/2019/06/05/对抗训练/对抗训练例子.png">
  <link rel="canonical" href="https://yuanxiaosc.github.io/2019/06/05/对抗训练/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>对抗训练（Adversarial Training） | 望江人工智库</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?359fbde2215e8ede98cdd58478ab2c53";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">望江人工智库</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">人工智能</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="fa fa-search fa-fw"></i>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/yuanxiaosc" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yuanxiaosc.github.io/2019/06/05/对抗训练/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="袁宵">
      <meta itemprop="description" content="专注于人工智能领域研究，特别是深度学习。">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="望江人工智库">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">对抗训练（Adversarial Training）

          
        </h2>

        <div class="post-meta">
		  	  
			  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
			   

              
                
              

              <time title="创建时间：2019-06-05 15:30:00" itemprop="dateCreated datePublished" datetime="2019-06-05T15:30:00+08:00">2019-06-05</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-06-10 14:09:22" itemprop="dateModified" datetime="2019-06-10T14:09:22+08:00">2019-06-10</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文/" itemprop="url" rel="index"><span itemprop="name">论文</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文/对抗训练/" itemprop="url" rel="index"><span itemprop="name">对抗训练</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="对抗训练基础知识"><a href="#对抗训练基础知识" class="headerlink" title="对抗训练基础知识"></a>对抗训练基础知识</h2><h3 id="对抗样本定义"><a href="#对抗样本定义" class="headerlink" title="对抗样本定义"></a>对抗样本定义</h3><p><img src="/2019/06/05/对抗训练/对抗训练例子.png" alt=""><br>对抗样本是使得机器学习的算法产生误判的样本，如上图所示，原有的模型以57.7%的置信度判定图片为熊猫，但添加微小的扰动后，模型以99.3%的置信度认为扰动后的图片是长臂猿。</p><h3 id="产生对抗样本的方法"><a href="#产生对抗样本的方法" class="headerlink" title="产生对抗样本的方法"></a>产生对抗样本的方法</h3><a id="more"></a>
<h4 id="基于梯度的方法"><a href="#基于梯度的方法" class="headerlink" title="基于梯度的方法"></a>基于梯度的方法</h4><p>$J(\theta;x;y)$是模型的损失函数，其中负梯度方向$-\nabla J_x(\theta;x;y)$是模型损失下降最快的方向，为了使$\hat x$对模型输出产生最大的改变，正梯度方向为扰动最大的方向，在该方向上进行扰动，可以快速产生对抗样本，该方法称为“快速符号梯度法”（fast gradient sign medthod，FGSM），见Ian J. Goodfellow 在2014发表的论文<a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">Explaining and Harnessing Adversarial Examples</a>。</p>
<h4 id="基于超平面分类"><a href="#基于超平面分类" class="headerlink" title="基于超平面分类"></a>基于超平面分类</h4><p>Deepfool是基于超平面分类思想的一种对抗样本生成方法。众所周知，在二分类问题中，超平面是实现分类的基础 ，那么要改变某个样本 x 的分类，最小的扰动就是将 x 挪到超平面上，这个距离的代价最小。多分类的问题也是类似。</p>
<p>显然我们希望模型可以变得更加鲁棒。一个最简单的方法，就是生成这些数据，并且把这些数据加入到训练数据中。这样模型就会正视这些数据，并且尽可能地拟合这些数据，最终完成了模型拟合，这些盲区也就覆盖住了。将对抗样本和原有数据一起进行训练，对抗样本产生的损失作为原损失的一部分，即在不修改原模型结构的情况下增加模型的损失，产生正则化的效果。</p>
<h3 id="对抗攻击"><a href="#对抗攻击" class="headerlink" title="对抗攻击"></a>对抗攻击</h3><p>对抗攻击指的是在模型原始输入上添加对抗扰动构建对抗样本从而使模型产生错误判断的过程。而在这一过程中，对抗扰动的选择和产生是关键。对抗扰动指的是在模型输入上添加能够误导模型输出的微小变。</p>
<p>虽然不同的文章对于对抗扰动的定义略有不同，但是一般来说对抗扰动具有两个特点：</p>
<ol>
<li>扰动是微小的甚至是肉眼难以观测到的；</li>
<li>添加的扰动必须有能力使得模型产生错误的输出。</li>
</ol>
<p>对抗攻击的分类</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>分类标准</th>
<th>名称</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>按对原始模型的访问权限不同</td>
<td>黑盒攻击与白盒攻击</td>
<td>白盒攻击指的是攻击者可以完全访问目标模型，他们可以了解模型的架构，参数和权重。黑盒攻击指的是攻击者很少或根本没有关于目标模型的知识，他们无法对其进行探测。在这种情况下，攻击者通常训练自己的模型并利用对抗性样本的可转移性来进行攻击。当然，白盒和黑盒攻击都无法改变模型和训练数据。在实际应用中，这两者的区别体现为：通过模型A来生成对抗样本，进而攻击模型B。当模型A与模型B是一个模型时，为白盒攻击；当模型A与模型B不为一个模型时，则为黑盒攻击。</td>
</tr>
<tr>
<td>按攻击目的</td>
<td>目标攻击和非目标攻击</td>
<td>目标攻击指的是生成的对抗样本希望被模型错分到某个特定的类别上。非目标攻击指的是对抗样本只要能让模型分错就行，不论错分到哪个类别都可以。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="常见对抗样本防御方法"><a href="#常见对抗样本防御方法" class="headerlink" title="常见对抗样本防御方法"></a>常见对抗样本防御方法</h3><p>由于目前防御方法仍然没有一个权威的分类方式，故笔者将目前看到过的一些防御方法大致分为以下四类：对抗训练、梯度掩码、随机化、去噪等。</p>
<ul>
<li>对抗训练：对抗训练旨在从随机初始化的权重中训练一个鲁棒的模型，其训练集由真实数据集和加入了对抗扰动的数据集组成，因此叫做对抗训练。</li>
<li>梯度掩码：由于当前的许多对抗样本生成方法都是基于梯度去生成的，所以如果将模型的原始梯度隐藏起来，就可以达到抵御对抗样本攻击的效果。</li>
<li>随机化：向原始模型引入随机层或者随机变量。使模型具有一定随机性，全面提高模型的鲁棒性，使其对噪声的容忍度变高。</li>
<li>去噪：在输入模型进行判定之前，先对当前对抗样本进行去噪，剔除其中造成扰动的信息，使其不能对模型造成攻击。</li>
</ul>
<h3 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h3><p>对抗训练是 Ian J. Goodfellow 在 <a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">Explaining and Harnessing Adversarial Examples</a> 最早提出来的一个对抗样本的防御方法。它的主要思想是：在模型训练过程中，训练样本不再只是原始样本，而是原始样本加上对抗样本，就相当于把产生的对抗样本当作新的训练样本加入到训练集中，对它们一视同仁，那么随着模型越来越多的训练，一方面原始图片的准确率会增加，另一方面，模型对对抗样本的鲁棒性也会增加。</p>
<p><a href="https://arxiv.org/abs/1808.06876" target="_blank" rel="noopener">Adversarial training for multi-context joint entity and relation extraction</a> 内容：<br>Adversarial training (AT) (Goodfellow et al., 2015) has been proposed to make classiﬁers more robust to input perturbations in the context of image recognition. In the context of NLP, several variants have been proposed for different tasks such as text classiﬁcation (Miyato et al., 2017), relation extraction (Wu et al., 2017) and POS tagging (Yasunaga et al., 2018). AT is considered as a regularization method. Unlike other regu- larization methods (i.e., dropout (Srivastava et al., 2014), word dropout (Iyyer et al., 2015)) that introduce random noise, AT generates perturbations that are variations of examples easily misclassiﬁed by the model.</p>
<p>对抗训练指的是在模型的训练过程中构建对抗样本并将对抗样本和原始样本混合一起训练模型的方法，换句话说就是在模型训练的过程中对模型进行对抗攻击从而提升模型对于对抗攻击的鲁棒性（也称为防御能力）。</p>
<p><strong>对抗训练局限性</strong><br>对抗训练（以及集成对抗训练）确实是防御对抗样本攻击的有效方法，但是它也存在着局限性。对抗训练是通过不断输入新类型的对抗样本进行训练，从而不断提升模型的鲁棒性。为了保证有效性，该方法需要使用高强度的对抗样本，并且网络架构要有充足的表达能力。而且无论添加多少对抗样本，都存在新的对抗样本可以欺骗网络。</p>
<h3 id="对抗攻击例子"><a href="#对抗攻击例子" class="headerlink" title="对抗攻击例子"></a>对抗攻击例子</h3><p><a href="https://arxiv.org/abs/1801.04354" target="_blank" rel="noopener">Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers</a> 中例子：<br><img src="/2019/06/05/对抗训练/2018_Gao_Figure_2.png" alt=""></p>
<p>part1 指的是原始的输入文本，part2 指的是对原始数据进行离散扰动后的文本，虽然只有少量字符被修改但是模型产生了完全不同的输出。在文本处理中，对抗扰动的特征 1 要求添加扰动后产生的对抗样本与原样本在语义上保持一致，即添加的扰动应该尽量不改变原始句子的语义。</p>
<h3 id="MNIST-实验"><a href="#MNIST-实验" class="headerlink" title="MNIST 实验"></a>MNIST 实验</h3><p>如下是在MNIST数据集上进行对抗训练的实验结果，红色线表示使用了对抗训练数据，蓝色表示没有使用。<br><img src="/2019/06/05/对抗训练/MNIST_Adversarial_Training_1.png" alt=""><br><img src="/2019/06/05/对抗训练/MNIST_Adversarial_Training_2.png" alt=""></p>
<p>结论：</p>
<ol>
<li>在MNIST训练集上，使用对抗训练会的模型损失增大，但是可以保持准确率与不使用对抗训练模型一致；</li>
<li>在MNIST验证集上，使用对抗训练的模型损比不使用对抗训练的模型损失更小、准确率更高。</li>
</ol>
<h3 id="常见问题解答"><a href="#常见问题解答" class="headerlink" title="常见问题解答"></a>常见问题解答</h3><p><strong>对模型正则化以后是不是仍然用原来的对抗样本来做实验？</strong><br>对模型正则化以后是不是仍然用原来的对抗样本来做实验，如果是那就没有意义，因为模型参数改变了，对抗样本应该重新生成；如果不是，那很难理解，因为模型的线性特性并没有改变，仍然可以找到对抗样本，没有理由错误率会降低。我觉得这里可以这么解释为什么重新生成对抗样本，错误率还是降低了。因为对于强正则化，模型的权重会变得比较小，输入扰动对模型的输出影响不仅取决于它本身，还与模型权重有关，既然加入了惩罚项对样本的扰动进行惩罚，那么模型就会降低权重来减小扰动带来的损失。</p>
<p><strong>实验结果表明对抗训练确实大幅提升了模型对白盒攻击的鲁棒性，但是对于黑盒攻击，甚至出现了比原始模型更高的错误率？</strong><br>因为对抗训练是针对某一个模型产生的对抗样本进行学习，那么模型势必会更具有针对性，所以就可能在面对其他模型生成的对抗样本攻击时会出现比原始模型更高的错误率。另外，还可以通过实验结果发现各个模型普遍对于对抗模型产生的对抗样本具有好的鲁棒性。这个现象验证了作者提出的一个观点，即对抗训练不仅仅是拟合了对模型有影响的扰动，其同时弱化了单步攻击时需要依赖的模型的线性假设，因此造成了使用单步攻击时的效果变差。为了进一步提升模型对黑盒攻击的鲁棒性，作者将生成对抗样本的模型从单个变成了多个，增加了对抗样本的多样性，削弱对抗训练时对单个模型的过拟合。观察实验结果发现，集成对抗训练对于白盒攻击的鲁棒性不如对抗训练，这是由于对抗训练增强的数据集恰恰就是白盒攻击的数据集，所以对白盒攻击的鲁棒性会更强，如果集成对抗训练使用的模型越多，则对白盒攻击的鲁棒性越差。但是，在黑盒攻击中，集成对抗训练表现出了很强的鲁棒性。</p>
<h3 id="对抗训练相关阅读资料"><a href="#对抗训练相关阅读资料" class="headerlink" title="对抗训练相关阅读资料"></a>对抗训练相关阅读资料</h3><div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://blog.csdn.net/cdpac/article/details/53170940" target="_blank" rel="noopener">对抗样本与对抗训练</a></td>
<td>科普浅析</td>
<td>20161207</td>
</tr>
<tr>
<td><a href="https://zhuanlan.zhihu.com/c_170476465" target="_blank" rel="noopener"><eyd与机器学习>：对抗攻击基础知识</eyd与机器学习></a></td>
<td>知乎专栏，讲解细致</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p><strong>这部分内容主要是有关对抗学习理论的研究</strong></p>
<blockquote>
<p><a href="https://github.com/thunlp/TAADpapers" target="_blank" rel="noopener">关于文本对抗性攻击和防御的必读文章</a></p>
</blockquote>
<h3 id="2014-Explaining-and-Harnessing-Adversarial-Examples"><a href="#2014-Explaining-and-Harnessing-Adversarial-Examples" class="headerlink" title="2014 Explaining and Harnessing Adversarial Examples"></a>2014 <a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">Explaining and Harnessing Adversarial Examples</a></h3><blockquote>
<p>Several machine learning models, including neural networks, consistently misclassify adversarial examples—-inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.</p>
<p>包括神经网络在内的几种机器学习模型始终错误地分类对抗性示例 - 通过对数据集中的示例应用小但有意的最坏情况扰动而形成的输入，使得扰动的输入导致模型以高置信度输出不正确的答案。 早期解释这种现象的尝试集中在非线性和过度拟合上。 我们认为神经网络易受对抗性扰动的主要原因是它们的线性特性。 这个解释得到了新的定量结果的支持，同时首先解释了关于它们的最有趣的事实：它们跨架构和训练集的泛化。 此外，该视图产生了一种生成对抗性示例的简单快速的方法。 使用此方法为对抗训练提供示例，我们减少了MNIST数据集上maxout网络的测试集错误。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">Explaining and Harnessing Adversarial Examples</a></td>
<td>论文原文</td>
<td>20141220</td>
</tr>
<tr>
<td><a href="https://blog.csdn.net/SYSU_BOND/article/details/79785989" target="_blank" rel="noopener">论文阅读:Explaining and Harnessing Adversarial Examples(解释分析对抗样本)</a></td>
<td>论文解析</td>
<td>20180402</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2015-Distributional-Smoothing-with-Virtual-Adversarial-Training"><a href="#2015-Distributional-Smoothing-with-Virtual-Adversarial-Training" class="headerlink" title="2015 Distributional Smoothing with Virtual Adversarial Training"></a>2015 <a href="https://arxiv.org/abs/1507.00677" target="_blank" rel="noopener">Distributional Smoothing with Virtual Adversarial Training</a></h3><p>由于文本不同于图片，文本一般转化为Index，通过Embedding后输入到network中，无法对input进行直接的求导，Miyato3 等提出将扰动添加到Embedding层，经过扰动后的Embedding对扰动有更好的鲁棒性。其模型结构为<br><img src="/2019/06/05/对抗训练/扰动样本在NLP领域的应用.png" alt=""></p>
<blockquote>
<p>We propose local distributional smoothness (LDS), a new notion of smoothness for statistical model that can be used as a regularization term to promote the smoothness of the model distribution. We named the LDS based regularization as virtual adversarial training (VAT). The LDS of a model at an input datapoint is defined as the KL-divergence based robustness of the model distribution against local perturbation around the datapoint. VAT resembles adversarial training, but distinguishes itself in that it determines the adversarial direction from the model distribution alone without using the label information, making it applicable to semisupervised learning. The computational cost for VAT is relatively low. For neural network, the approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations. When we applied our technique to supervised and semi-supervised learning for the MNIST dataset, it outperformed all the training methods other than the current state of the art method, which is based on a highly advanced generative model. We also applied our method to SVHN and NORB, and confirmed our method’s superior performance over the current state of the art semi-supervised method applied to these datasets.</p>
</blockquote>
<h3 id="2017-Adversarial-Dropout-for-Supervised-and-Semi-supervised-Learning"><a href="#2017-Adversarial-Dropout-for-Supervised-and-Semi-supervised-Learning" class="headerlink" title="2017 Adversarial Dropout for Supervised and Semi-supervised Learning"></a>2017 <a href="https://arxiv.org/abs/1707.03631" target="_blank" rel="noopener">Adversarial Dropout for Supervised and Semi-supervised Learning</a></h3><blockquote>
<p>Recently, the training with adversarial examples, which are generated by adding a small but worst-case perturbation on input examples, has been proved to improve generalization performance of neural networks. In contrast to the individually biased inputs to enhance the generality, this paper introduces adversarial dropout, which is a minimal set of dropouts that maximize the divergence between the outputs from the network with the dropouts and the training supervisions. The identified adversarial dropout are used to reconfigure the neural network to train, and we demonstrated that training on the reconfigured sub-network improves the generalization performance of supervised and semi-supervised learning tasks on MNIST and CIFAR-10. We analyzed the trained model to reason the performance improvement, and we found that adversarial dropout increases the sparsity of neural networks more than the standard dropout does.</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://arxiv.org/abs/1707.03631" target="_blank" rel="noopener">Adversarial Dropout for Supervised and Semi-supervised Learning</a></td>
<td>论文原文</td>
<td>20170712</td>
</tr>
<tr>
<td><a href="https://blog.csdn.net/qingmengshuo/article/details/83146282" target="_blank" rel="noopener">对抗训练-Adversarial Training for Supervised and Semi-Supervised Learning（对抗训练在监督和半监督学习上的应用）</a></td>
<td>论文解析</td>
<td>20181018</td>
</tr>
<tr>
<td><a href="https://zhuanlan.zhihu.com/p/47831479" target="_blank" rel="noopener">半监督文本分类的对抗训练</a></td>
<td>论文解析</td>
<td>20181231</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2018-Certifying-Some-Distributional-Robustness-with-Principled-Adversarial-Training"><a href="#2018-Certifying-Some-Distributional-Robustness-with-Principled-Adversarial-Training" class="headerlink" title="2018 Certifying Some Distributional Robustness with Principled Adversarial Training"></a>2018 <a href="https://arxiv.org/abs/1710.10571v4" target="_blank" rel="noopener">Certifying Some Distributional Robustness with Principled Adversarial Training</a></h3><blockquote>
<p>深度学习顶会“无冕之王”ICLR 2018评审结果出炉，斯坦福大学对抗训练研究得分第一。</p>
<p>Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations. By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.</p>
<p>神经网络容易受到对抗性的影响，研究人员提出了许多启发式攻击和防御机制。 我们通过分布式鲁棒优化的原理镜头来解决这个问题，这保证了在对抗性输入扰动下的性能。 通过考虑拉格朗日惩罚公式扰乱 Wasserstein balls 的基础数据分布，我们提供了一个训练程序，增加了模型参数更新与最坏情况下的训练数据扰动。 对于平滑损失，我们的程序可证明实现了中等水平的稳健性，与经验风险最小化相比，计算或统计成本很低。 此外，我们的统计保证使我们能够有效地证明 population 损失的稳健性。 对于难以察觉的扰动，我们的方法匹配或优于启发式方法。</p>
</blockquote>
<h3 id="2019-Towards-a-Robust-Deep-Neural-Network-in-Text-Domain-A-Survey"><a href="#2019-Towards-a-Robust-Deep-Neural-Network-in-Text-Domain-A-Survey" class="headerlink" title="2019 Towards a Robust Deep Neural Network in Text Domain A Survey"></a>2019 <a href="https://arxiv.org/abs/1902.07285" target="_blank" rel="noopener">Towards a Robust Deep Neural Network in Text Domain A Survey</a></h3><blockquote>
<p>Deep neural networks (DNNs) have shown an inherent vulnerability to adversarial examples which are maliciously crafted on real examples by attackers, aiming at making target DNNs misbehave. The threats of adversarial examples are widely existed in image, voice, speech, and text recognition and classification. Inspired by the previous work, researches on adversarial attacks and defenses in text domain develop rapidly. In order to make people have a general understanding about the field, this article presents a comprehensive review on adversarial examples in text. We analyze the advantages and shortcomings of recent adversarial examples generation methods and elaborate the efficiency and limitations on countermeasures. Finally, we discuss the challenges in adversarial texts and provide a research direction of this aspect.</p>
<p>深度神经网络（DNN）已显示出对抗性示例的固有漏洞，这些示例是攻击者在真实示例中恶意制作的，旨在使目标DNN行为不端。 对抗性示例的威胁在图像，语音，语音，文本识别和分类中广泛存在。 受以往工作的启发，对文本域中的对抗性攻击和防御的研究迅速发展。 为了使人们对该领域有一个大致的了解，本文对文本中的对抗性例子进行了全面的回顾。 我们分析了近期对抗性实例生成方法的优缺点，并详细阐述了对策的效率和局限性。 最后，我们讨论对抗性文本中的挑战，并提供这方面的研究方向。</p>
</blockquote>
<hr>
<p><strong>这部分内容是对抗训练应用于各个模型</strong></p>
<h3 id="2017-Multi-Domain-Adversarial-Learning-for-Slot-Filling-in-Spoken-Language-Understanding"><a href="#2017-Multi-Domain-Adversarial-Learning-for-Slot-Filling-in-Spoken-Language-Understanding" class="headerlink" title="2017 Multi-Domain Adversarial Learning for Slot Filling in Spoken Language Understanding"></a>2017 <a href="https://arxiv.org/abs/1711.11310" target="_blank" rel="noopener">Multi-Domain Adversarial Learning for Slot Filling in Spoken Language Understanding</a></h3><blockquote>
<p>对抗训练应用槽填充任务，这里使用对抗训练主要是为了训练出一个通用表示的槽填充模型，然后将这个模型的表示与特定领域模型的表示结合起来做预测，作用是减少了特定领域标签数据和提升模型效果。</p>
</blockquote>
<h3 id="2018-Adversarial-training-for-multi-context-joint-entity-and-relation-extraction"><a href="#2018-Adversarial-training-for-multi-context-joint-entity-and-relation-extraction" class="headerlink" title="2018 Adversarial training for multi-context joint entity and relation extraction"></a>2018 <a href="https://arxiv.org/abs/1808.06876" target="_blank" rel="noopener">Adversarial training for multi-context joint entity and relation extraction</a></h3><blockquote>
<p>本文是 <a href="https://arxiv.org/abs/1804.07847" target="_blank" rel="noopener">Joint entity recognition and relation extraction as a multi-head selection problem</a> 的姊妹篇，都是同一个多头选择实体关系抽取模型，只是增加了对抗训练这个神经网络正则化方法。</p>
</blockquote>

    </div>

    
    
    
        
      
        <div id="reward-container">
  <div>本站所有文章和源码均免费开放，如您喜欢，可以请我喝杯咖啡</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.jpg" alt="袁宵 微信支付">
        <p>微信支付</p>
      </div>
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="袁宵 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>袁宵</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yuanxiaosc.github.io/2019/06/05/对抗训练/" title="对抗训练（Adversarial Training）">https://yuanxiaosc.github.io/2019/06/05/对抗训练/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/论文/" rel="tag"># 论文</a>
            
              <a href="/tags/对抗训练/" rel="tag"># 对抗训练</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/06/04/EfficientNet_Rethinking_Model_Scaling_for_Convolutional_Neural_Networks/" rel="next" title="EfficientNet Rethinking Model Scaling for Convolutional Neural Networks">
                  <i class="fa fa-chevron-left"></i> EfficientNet Rethinking Model Scaling for Convolutional Neural Networks
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/06/10/Multi-Task_Deep_Neural_Networks/" rel="prev" title="Multi-Task Deep Neural Networks (MT-DNN)">
                  Multi-Task Deep Neural Networks (MT-DNN) <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#对抗训练基础知识"><span class="nav-number">1.</span> <span class="nav-text">对抗训练基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#对抗样本定义"><span class="nav-number">1.1.</span> <span class="nav-text">对抗样本定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#产生对抗样本的方法"><span class="nav-number">1.2.</span> <span class="nav-text">产生对抗样本的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于梯度的方法"><span class="nav-number">1.2.1.</span> <span class="nav-text">基于梯度的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基于超平面分类"><span class="nav-number">1.2.2.</span> <span class="nav-text">基于超平面分类</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对抗攻击"><span class="nav-number">1.3.</span> <span class="nav-text">对抗攻击</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见对抗样本防御方法"><span class="nav-number">1.4.</span> <span class="nav-text">常见对抗样本防御方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对抗训练"><span class="nav-number">1.5.</span> <span class="nav-text">对抗训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对抗攻击例子"><span class="nav-number">1.6.</span> <span class="nav-text">对抗攻击例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MNIST-实验"><span class="nav-number">1.7.</span> <span class="nav-text">MNIST 实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见问题解答"><span class="nav-number">1.8.</span> <span class="nav-text">常见问题解答</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对抗训练相关阅读资料"><span class="nav-number">1.9.</span> <span class="nav-text">对抗训练相关阅读资料</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2014-Explaining-and-Harnessing-Adversarial-Examples"><span class="nav-number">1.10.</span> <span class="nav-text">2014 Explaining and Harnessing Adversarial Examples</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2015-Distributional-Smoothing-with-Virtual-Adversarial-Training"><span class="nav-number">1.11.</span> <span class="nav-text">2015 Distributional Smoothing with Virtual Adversarial Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2017-Adversarial-Dropout-for-Supervised-and-Semi-supervised-Learning"><span class="nav-number">1.12.</span> <span class="nav-text">2017 Adversarial Dropout for Supervised and Semi-supervised Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2018-Certifying-Some-Distributional-Robustness-with-Principled-Adversarial-Training"><span class="nav-number">1.13.</span> <span class="nav-text">2018 Certifying Some Distributional Robustness with Principled Adversarial Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-Towards-a-Robust-Deep-Neural-Network-in-Text-Domain-A-Survey"><span class="nav-number">1.14.</span> <span class="nav-text">2019 Towards a Robust Deep Neural Network in Text Domain A Survey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2017-Multi-Domain-Adversarial-Learning-for-Slot-Filling-in-Spoken-Language-Understanding"><span class="nav-number">1.15.</span> <span class="nav-text">2017 Multi-Domain Adversarial Learning for Slot Filling in Spoken Language Understanding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2018-Adversarial-training-for-multi-context-joint-entity-and-relation-extraction"><span class="nav-number">1.16.</span> <span class="nav-text">2018 Adversarial training for multi-context joint entity and relation extraction</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.png"
      alt="袁宵">
  <p class="site-author-name" itemprop="name">袁宵</p>
  <div class="site-description" itemprop="description">专注于人工智能领域研究，特别是深度学习。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives">
        
          <span class="site-state-item-count">141</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">132</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/yuanxiaoSC" title="GitHub &rarr; https://github.com/yuanxiaoSC" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:wangzichaochaochao@gmail.com" title="E-Mail &rarr; mailto:wangzichaochaochao@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>
  <div class="cc-license motion-element" itemprop="license">
    
  
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
	  

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">袁宵</span>
</div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5d9c4b1ac4deb418" async="async"></script>
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">全站共 400k 字</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/pisces.js?v=7.4.1"></script>

<script src="/js/next-boot.js?v=7.4.1"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>








  <script src="/js/local-search.js?v=7.4.1"></script>














  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
