<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.7.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-32x32.ico?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="baidu-site-verification" content="eYmWT0dEmt">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":true,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="必读语言模型推荐Github PLMpapers Must-read Papers on pre-trained language models. 词语嵌入和语言模型基本概念 动手体验词嵌入 http://projector.tensorflow.org/">
<meta name="keywords" content="Embedding,词嵌入">
<meta property="og:type" content="article">
<meta property="og:title" content="Embedding 和语言模型 LanguageModel">
<meta property="og:url" content="https://yuanxiaosc.github.io/2019/01/07/词嵌入Embedding和语言模型LanguageModel/index.html">
<meta property="og:site_name" content="望江人工智库">
<meta property="og:description" content="必读语言模型推荐Github PLMpapers Must-read Papers on pre-trained language models. 词语嵌入和语言模型基本概念 动手体验词嵌入 http://projector.tensorflow.org/">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/thunlp/PLMpapers/master/PLMfamily.jpg">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/01/07/词嵌入Embedding和语言模型LanguageModel/one-hot.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/01/07/词嵌入Embedding和语言模型LanguageModel/embedding2.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/01/07/词嵌入Embedding和语言模型LanguageModel/d2.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/01/07/词嵌入Embedding和语言模型LanguageModel/d1.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/01/07/词嵌入Embedding和语言模型LanguageModel/c1.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/01/07/词嵌入Embedding和语言模型LanguageModel/a1.png">
<meta property="og:updated_time" content="2019-10-16T12:33:51.920Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Embedding 和语言模型 LanguageModel">
<meta name="twitter:description" content="必读语言模型推荐Github PLMpapers Must-read Papers on pre-trained language models. 词语嵌入和语言模型基本概念 动手体验词嵌入 http://projector.tensorflow.org/">
<meta name="twitter:image" content="https://raw.githubusercontent.com/thunlp/PLMpapers/master/PLMfamily.jpg">
  <link rel="canonical" href="https://yuanxiaosc.github.io/2019/01/07/词嵌入Embedding和语言模型LanguageModel/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Embedding 和语言模型 LanguageModel | 望江人工智库</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?359fbde2215e8ede98cdd58478ab2c53";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">望江人工智库</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">TF-KMP</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="fa fa-search fa-fw"></i>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/yuanxiaosc" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yuanxiaosc.github.io/2019/01/07/词嵌入Embedding和语言模型LanguageModel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="袁宵">
      <meta itemprop="description" content="专注于机器学习前沿论文（技术）研究和应用，欢迎邮件交流。">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="望江人工智库">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">Embedding 和语言模型 LanguageModel

          
        </h2>

        <div class="post-meta">
		  	  
			  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
			   

              
                
              

              <time title="创建时间：2019-01-07 09:30:15" itemprop="dateCreated datePublished" datetime="2019-01-07T09:30:15+08:00">2019-01-07</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-10-16 20:33:51" itemprop="dateModified" datetime="2019-10-16T20:33:51+08:00">2019-10-16</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="必读语言模型推荐"><a href="#必读语言模型推荐" class="headerlink" title="必读语言模型推荐"></a>必读语言模型推荐</h2><p>Github <a href="https://github.com/thunlp/PLMpapers" target="_blank" rel="noopener">PLMpapers</a></p><blockquote>
<p>Must-read Papers on pre-trained language models.</p>
</blockquote><p><img src="https://raw.githubusercontent.com/thunlp/PLMpapers/master/PLMfamily.jpg" alt=""></p><h2 id="词语嵌入和语言模型基本概念"><a href="#词语嵌入和语言模型基本概念" class="headerlink" title="词语嵌入和语言模型基本概念"></a>词语嵌入和语言模型基本概念</h2><blockquote>
<p>动手体验词嵌入 <a href="http://projector.tensorflow.org/" target="_blank" rel="noopener">http://projector.tensorflow.org/</a></p>
</blockquote><a id="more"></a>



<h3 id="为什么要使用Word-Embedding"><a href="#为什么要使用Word-Embedding" class="headerlink" title="为什么要使用Word Embedding"></a>为什么要使用Word Embedding</h3><p>在信号处理领域，图像和音频信号的输入往往是表示成高维度、密集的向量形式，在图像和音频的应用系统中，如何对输入信息进行编码(Encoding)显得非常重要和关键，这将直接决定了系统的质量。然而，在自然语言处理领域中，传统的做法是将词表示成离散的符号，例如将 [cat] 表示为 [Id537]，而 [dog] 表示为 [Id143]。这样做的缺点在于，没有提供足够的信息来体现词语之间的某种关联，例如尽管cat和dog不是同一个词，但是却应该有着某种的联系（如都是属于动物种类）。由于这种一元表示法(One-hot Representation)使得词向量过于稀疏，所以往往需要大量的语料数据才能训练出一个令人满意的模型。而Word Embedding技术则可以解决上述传统方法带来的问题。</p>
<p>推荐阅读Word2vec的文章 <a href="https://jalammar.github.io/illustrated-word2vec/" target="_blank" rel="noopener">The Illustrated Word2vec</a></p>
<h3 id="对比表示文本的各种方法"><a href="#对比表示文本的各种方法" class="headerlink" title="对比表示文本的各种方法"></a>对比表示文本的各种方法</h3><p>机器学习模型将矢量（数字数组）作为输入。在处理文本时，我们必须做的第一件事就是在将字符串转换为数字之前将字符串转换为数字（或“向量化”文本），然后再将其提供给模型。在本节中，我们将介绍这样做的三种策略。</p>
<h4 id="One-hot-encodings"><a href="#One-hot-encodings" class="headerlink" title="One-hot encodings"></a>One-hot encodings</h4><p>作为第一个想法，我们可能会“One-hot”地编码我们词汇表中的每个单词。考虑句子“The cat sat on the mat”。 这句话中的词汇（或唯一的单词）是（cat，mat，on，sat，the）。为了表示每个单词，我们将创建一个长度等于词汇表的零向量，然后在与该单词对应的索引中设置为一。 此方法如下图所示。<br><img src="/2019/01/07/词嵌入Embedding和语言模型LanguageModel/one-hot.png" alt=""><br>为了创建包含句子编码的向量，我们可以连接每个单词的One-hot向量。</p>
<p>缺点：这种方法效率低下。 One-hot编码矢量是稀疏的（意味着，大多数indicices为零）。想象一下，我们在词汇表中有10,000个单词。为了对每个单词进行One-hot编码，我们将创建一个向量，其中99.99％的元素为零。</p>
<h4 id="使用唯一编号对每个单词进行编码"><a href="#使用唯一编号对每个单词进行编码" class="headerlink" title="使用唯一编号对每个单词进行编码"></a>使用唯一编号对每个单词进行编码</h4><p>我们可能尝试的第二种方法是使用唯一编号对每个单词进行编码。继续上面的例子，我们可以将1分配给“cat”，将2分配给“mat”，依此类推。然后我们可以将句子“The cat sat on the mat”编码为像[5,1,4,3,5,2]这样的密集向量。这个应用程序是有效的。我们现在有一个密集的（所有元素都已满），而不是稀疏的向量。</p>
<p>但是，这种方法有两个缺点：</p>
<ol>
<li>整数编码是任意的（它不捕获单词之间的任何关系）。</li>
<li>对于要解释的模型，整数编码可能具有挑战性。例如，线性分类器为每个特征学习单个权重。因为任何两个单词的相似性与其编码的相似性之间没有关系，所以这个特征-权重组合没有意义。</li>
</ol>
<h4 id="Word-embeddings-词嵌入"><a href="#Word-embeddings-词嵌入" class="headerlink" title="Word embeddings 词嵌入"></a>Word embeddings 词嵌入</h4><p>词嵌入为我们提供了一种使用高效，密集表示的方法，其中类似的单词具有相似的编码。 重要的是，我们不必手动指定此编码。 嵌入是浮点值的密集向量（向量的长度是您指定的参数）。它们不是手动指定嵌入的值，而是可训练的参数（在训练期间由模型学习的权重，与模型学习密集层的权重的方式相同）。通常会看到8维（对于小数据集）的单词嵌入，在处理大型数据集时最多可达1024维。 更高维度的嵌入可以捕获单词之间的细粒度关系，但需要更多的数据来学习。<br><img src="/2019/01/07/词嵌入Embedding和语言模型LanguageModel/embedding2.png" alt=""></p>
<p>缺点：词嵌入没有上下文的概念，一个词语与一个词嵌入向量一一对应，无法解决一词多义问题。</p>
<h2 id="词嵌入资源"><a href="#词嵌入资源" class="headerlink" title="词嵌入资源"></a>词嵌入资源</h2><div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://ai.tencent.com/ailab/nlp/embedding.html" target="_blank" rel="noopener">Tencent AI Lab Embedding Corpus for Chinese Words and Phrases</a></td>
<td>This corpus provides 200-dimension vector representations, a.k.a. embeddings, for over 8 million Chinese words and phrases, which are pre-trained on large-scale high-quality data. These vectors, capturing semantic meanings for Chinese words and phrases, can be widely applied in many downstream Chinese processing tasks (e.g., named entity recognition and text classification) and in further research.<a href="https://www.jiqizhixin.com/articles/2018-10-19-16" target="_blank" rel="noopener">腾讯AI Lab开源大规模高质量中文词向量数据，800万中文词随你用</a></td>
</tr>
<tr>
<td><a href="https://github.com/Hironsan/awesome-embedding-models" target="_blank" rel="noopener">awesome-embedding-models</a></td>
<td>精选的嵌入模型教程，项目和社区的精选列表。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="向量空间模型"><a href="#向量空间模型" class="headerlink" title="向量空间模型"></a>向量空间模型</h3><p>向量空间模型(Vector space models, VSMs)将词语表示为一个连续的词向量，并且语义接近的词语对应的词向量在空间上也是接近的。VSMs在NLP中拥有很长的历史，但是所有的方法在某种程度上都是基于一种分布式假说，该假说的思想是如果两个词的上下文(context)相同，那么这两个词所表达的语义也是一样的；换言之，两个词的语义是否相同或相似，取决于两个词的上下文内容，上下文相同表示两个词是可以等价替换的。</p>
<p>基于分布式假说理论的词向量生成方法主要分两大类：计数法(count-based methods, e.g. Latent Semantic Analysis)和预测法(predictive methods, e.g. neural probabilistic language models)。<a href="http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf" target="_blank" rel="noopener">Baroni等人</a>详细论述了这两种方法的区别，简而言之，计数法是在大型语料中统计词语及邻近的词的共现频率，然后将之为每个词都映射为一个稠密的向量表示；预测法是直接利用词语的邻近词信息来得到预测词的词向量（词向量通常作为模型的训练参数）。</p>
<h3 id="文本问题"><a href="#文本问题" class="headerlink" title="文本问题"></a>文本问题</h3><p>机器学习这样的技术比较喜欢被定义好的固定长度的输入和输出，因此不固定输入输出是文本建模的一个问题。</p>
<p>机器学习算法不能直接处理原始文本，文本必须转换成数字。具体来说，是数字的向量。</p>
<p>“在语言处理中，向量x是由文本数据派生而来的，以反映文本的各种语言属性。”在自然语言处理中神经网络方法,2017年。</p>
<p>这被称为特征提取或特征编码。这是一种流行的、简单的文本数据提取方法被称为文本的词汇模型。</p>
<h3 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h3><p><a href="https://baike.baidu.com/item/%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B/22776998?fr=aladdin" target="_blank" rel="noopener">词袋模型</a>下，像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式不考虑文法以及词的顺序。最近词袋模型也被应用在电脑视觉领域 。<br>词袋模型被广泛应用在文件分类，词出现的频率可以用来当作训练分类器的特征。<br><img src="/2019/01/07/词嵌入Embedding和语言模型LanguageModel/d2.png" alt=""></p>
<h3 id="词向量模型"><a href="#词向量模型" class="headerlink" title="词向量模型"></a>词向量模型</h3><h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p><a href="https://baike.baidu.com/item/Word2vec/22660840?fr=aladdin" target="_blank" rel="noopener">词向量</a>具有良好的语义特性，是表示词语特征的常用方式。词向量每一维的值代表一个具有一定的语义和语法上解释的特征。所以，可以将词向量的每一维称为一个词语特征。词向量具有多种形式，distributed representation 是其中一种。一个 distributed representation 是一个稠密、低维的实值向量。distributed representation 的每一维表示词语的一个潜在特征，该特 征捕获了有用的句法和语义特性。可见 ，distributed representation 中的 distributed 一词体现了词向量这样一个特点：将词语的不同句法和语义特征分布到它的每一个维度去表示 。</p>
<p>自然语言是一套用来表达含义的复杂系统。在这套系统中，词是表义的基本单元。顾名思义，词向量是用来表示词的向量，也可被认为是词的特征向量。</p>
<p>这通常需要把维数为词典大小的高维空间嵌入到一个更低维数的连续向量空间。把词映射为实数域上向量的技术也叫词嵌入（word embedding）。近年来，词向量已逐渐成为自然语言处理的基础知识。</p>
<p>语言模型旨在为语句的联合概率函数P(w1,…,wT)建模, 其中wi表示句子中的第i个词。语言模型的目标是，希望模型对有意义的句子赋予大概率，对没意义的句子赋予小概率。 这样的模型可以应用于很多领域，如机器翻译、语音识别、信息检索、词性标注、手写识别等，它们都希望能得到一个连续序列的概率。</p>
<h3 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a>统计语言模型</h3><p><img src="/2019/01/07/词嵌入Embedding和语言模型LanguageModel/d1.png" alt=""></p>
<h3 id="基于神经网络的分布表示"><a href="#基于神经网络的分布表示" class="headerlink" title="基于神经网络的分布表示"></a>基于神经网络的分布表示</h3><p>由于神经网络较为灵活,这类方法的最大优势在于可以表示复杂的上下文。更主要的原因是NNLM不再去老老实实地统计词频，而是通过模型拟合的方式逼近真实分布。在前面基于矩阵的分布表示方法中,最常用的上下文是词。如果使用包含词序信息的 n-gram 作为上下文,当 n 增加时,n-gram 的总数会呈指数级增长,此时会遇到维数灾难问题。而神经网络在表示 n-gram 时,可以通过一些组合方式对 n 个词进行组合,参数个数仅以线性速度增长。有了这一优势,神经网络模型可以对更复杂的上下文进行建模,在词向量中包含更丰富的语义信息。<br><strong><a href="https://www.jianshu.com/p/4bca99d40597" target="_blank" rel="noopener">基于NN的所有训练方法都是在训练语言模型(LM)的同时，顺便得到词向量的</a></strong>。</p>
<h3 id="Neural-net-language-models"><a href="#Neural-net-language-models" class="headerlink" title="Neural net language models"></a><a href="http://www.scholarpedia.org/article/Neural_net_language_models" target="_blank" rel="noopener">Neural net language models</a></h3><h3 id="word2vector"><a href="#word2vector" class="headerlink" title="word2vector"></a><a href="https://arxiv.org/abs/1301.3781v3" target="_blank" rel="noopener">word2vector</a></h3><div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
<th>附加</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://arxiv.org/abs/1301.3781v3" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a></td>
<td>word2vector 原文</td>
<td>20130116</td>
</tr>
<tr>
<td><a href="https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py" target="_blank" rel="noopener">models/tutorials/embedding/word2vec.py</a></td>
<td>官方实现</td>
<td>20171030</td>
</tr>
</tbody>
</table>
</div>
<h3 id="传统词嵌入的过程embedding-lookup"><a href="#传统词嵌入的过程embedding-lookup" class="headerlink" title="传统词嵌入的过程embedding_lookup()"></a>传统词嵌入的过程<a href="https://blog.csdn.net/u013041398/article/details/60955847" target="_blank" rel="noopener">embedding_lookup()</a></h3><p><img src="/2019/01/07/词嵌入Embedding和语言模型LanguageModel/c1.png" alt=""><br>词嵌入的过程，注意其中的维度变化只是一种可能的方式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_ids = tf.placeholder(dtype=tf.int32, shape=[<span class="keyword">None</span>])</span><br><span class="line"></span><br><span class="line">embedding = tf.Variable(np.identity(<span class="number">5</span>, dtype=np.float32))</span><br><span class="line">input_embedding = tf.nn.embedding_lookup(embedding, input_ids)</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">print(embedding.eval())</span><br><span class="line">print(sess.run(input_embedding, feed_dict=&#123;input_ids:[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]&#125;))</span><br></pre></td></tr></table></figure>
<p>代码中先使用palceholder定义了一个未知变量input_ids用于存储索引，和一个已知变量embedding，是一个5*5的对角矩阵。运行结果为,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">embedding = [[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">            [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">            [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">            [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line">            [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line">input_embedding = [[<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">                   [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">                   [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line">                   [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">                   [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line">                   [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">                   [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure></p>
<p>简单的讲就是根据input_ids中的id，寻找embedding中的对应元素。比如，input_ids=[1,3,5]，则找出embedding中下标为1,3,5的向量组成一个矩阵返回。</p>
<p>如果将input_ids改写成下面的格式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_ids = tf.placeholder(dtype=tf.int32, shape=[<span class="keyword">None</span>, <span class="keyword">None</span>])</span><br><span class="line"></span><br><span class="line">embedding = tf.Variable(np.identity(<span class="number">5</span>, dtype=np.float32))</span><br><span class="line">input_embedding = tf.nn.embedding_lookup(embedding, input_ids)</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">print(embedding.eval())</span><br><span class="line">print(sess.run(input_embedding, feed_dict=&#123;input_ids:[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">3</span>]]&#125;))</span><br></pre></td></tr></table></figure>
<p>输出结果就会变成如下的格式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input_embedding = [[[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">                    [<span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]]</span><br><span class="line">                  [[<span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">                    [<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br><span class="line">                  [[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">                    [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]]]</span><br></pre></td></tr></table></figure></p>
<p>对比上下两个结果不难发现，相当于在np.array中直接采用下标数组获取数据。需要注意的细节是返回的tensor的dtype和传入的被查询的tensor的dtype保持一致；和ids的dtype无关。</p>
<h3 id="字符嵌入"><a href="#字符嵌入" class="headerlink" title="字符嵌入"></a><a href="https://www.cnblogs.com/qwangxiao/p/8971924.html" target="_blank" rel="noopener">字符嵌入</a></h3><p>在NLP中，我们通常使用的过滤器会滑过整个矩阵(单词)。因此，过滤器的“宽度（width）”通常与输入矩阵的宽度相同。高度，或区域大小（region size），可能会有所不同，但是滑动窗口一次在2-5个字是典型的。</p>
<p>一个NLP上的卷积实例是下面这样：<br><img src="/2019/01/07/词嵌入Embedding和语言模型LanguageModel/a1.png" alt=""><br>上图展示了CNN在文本分类的使用，使用了2种过滤器（卷积核），每个过滤器有3种高度（区域大小），即有6种卷积结构（左起第2列），所以会产生6中卷积后的结果（左起第3列），经过最大池化层（后面还会提到池化层），每个卷积的结果将变为1个值（左起第4列），最终生成一个向量（左起第5列），最终经过分类器得到一个二分类结果（最后一列）。来自：<a href="https://arxiv.org/abs/1510.03820v4" target="_blank" rel="noopener">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</a>。</p>
<hr>
<h3 id="对比Word2vec-Fasttext-Glove-Elmo-Bert-Flair-pre-train-Word-Embedding"><a href="#对比Word2vec-Fasttext-Glove-Elmo-Bert-Flair-pre-train-Word-Embedding" class="headerlink" title="对比Word2vec, Fasttext, Glove, Elmo, Bert, Flair pre-train Word Embedding"></a>对比Word2vec, Fasttext, Glove, Elmo, Bert, Flair pre-train Word Embedding</h3><p><a href="https://github.com/zlsdu/Word-Embedding" target="_blank" rel="noopener">Word2vec, Fasttext, Glove, Elmo, Bert, Flair pre-train Word Embedding</a></p>
<hr>
<h2 id="词嵌入-2018历史综述-Bornstein"><a href="#词嵌入-2018历史综述-Bornstein" class="headerlink" title="词嵌入-2018历史综述 Bornstein"></a>词嵌入-2018历史综述 <a href="https://towardsdatascience.com/@aribornstein" target="_blank" rel="noopener">Bornstein</a></h2><p><a href="https://towardsdatascience.com/beyond-word-embeddings-part-1-an-overview-of-neural-nlp-milestones-82b97a47977f" target="_blank" rel="noopener">Beyond Word Embeddings Part 1</a><br><a href="https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec" target="_blank" rel="noopener">Beyond Word Embeddings Part 2</a><br><a href="https://towardsdatascience.com/beyond-word-embeddings-part-3-four-common-flaws-in-state-of-the-art-neural-nlp-models-c1d35d3496d0" target="_blank" rel="noopener">Beyond Word Embeddings Part 3</a><br><a href="https://towardsdatascience.com/beyond-word-embeddings-part-4-introducing-semantic-structure-to-neural-nlp-96cf8a2723fb" target="_blank" rel="noopener">Beyond Word Embeddings Part 4</a></p>
<h2 id="语言模型词嵌入的过程"><a href="#语言模型词嵌入的过程" class="headerlink" title="语言模型词嵌入的过程"></a>语言模型词嵌入的过程</h2><p>参看 <a href="https://allennlp.org/elmo" target="_blank" rel="noopener">allennlp.org - elmo</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
<th>附加</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://www.jianshu.com/p/a6bc14323d77" target="_blank" rel="noopener">词向量技术-从word2vec到ELMo</a></td>
<td></td>
<td>20180724</td>
</tr>
<tr>
<td><a href="https://www.jiqizhixin.com/articles/2018-07-09-9" target="_blank" rel="noopener">NLP领域的ImageNet时代到来：词嵌入「已死」，语言模型当立</a></td>
<td>翻译自 <a href="https://thegradient.pub/nlp-imagenet/" target="_blank" rel="noopener">NLP’s ImageNet moment has arrived</a></td>
<td>20180709</td>
</tr>
<tr>
<td><a href="https://www.jiqizhixin.com/articles/2018-06-06-4" target="_blank" rel="noopener">2018最好的词句嵌入技术概览：从无监督学习到监督、多任务学习</a></td>
<td>翻译自 <a href="https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a" target="_blank" rel="noopener">The Current Best of Universal Word Embeddings and Sentence Embeddings</a></td>
<td>20180606</td>
</tr>
<tr>
<td><a href="https://zhuanlan.zhihu.com/p/47488095" target="_blank" rel="noopener">NLP的游戏规则从此改写？从word2vec, ELMo到BERT</a></td>
<td><a href="https://www.zhihu.com/people/tsxiyao/activities" target="_blank" rel="noopener">夕小瑶</a> 详细解读</td>
<td></td>
</tr>
<tr>
<td><a href="https://www.jiqizhixin.com/articles/2018-12-13-30" target="_blank" rel="noopener">神经语言模型的最新进展</a></td>
<td>卡内基梅隆大学博士杨植麟受邀至清华大学计算机系进行主题为「神经语言模型的最新进展」的演讲。</td>
<td>20181213</td>
</tr>
</tbody>
</table>
</div>
<h2 id="自然语言处理中的语言模型预训练方法"><a href="#自然语言处理中的语言模型预训练方法" class="headerlink" title="自然语言处理中的语言模型预训练方法"></a><a href="https://zhuanlan.zhihu.com/p/47342053" target="_blank" rel="noopener">自然语言处理中的语言模型预训练方法</a></h2><p>语言模型评价</p>
<p>语言模型构造完成后，如何确定好坏呢？ 目前主要有两种评价方法：</p>
<p>实用方法：通过查看该模型在实际应用（如拼写检查、机器翻译）中的表现来评价，优点是直观、实用，缺点是缺乏针对性、不够客观；<br>理论方法：迷惑度/困惑度/混乱度（preplexity），其基本思想是给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好。</p>
<h2 id="深度长文：NLP的巨人肩膀（上）"><a href="#深度长文：NLP的巨人肩膀（上）" class="headerlink" title="深度长文：NLP的巨人肩膀（上）"></a><a href="https://www.jiqizhixin.com/articles/2018-12-10-17" target="_blank" rel="noopener">深度长文：NLP的巨人肩膀（上）</a></h2><blockquote>
<p>纵览2003-2018年自然语言处理中词语表示的发展史。</p>
</blockquote>
<p>本质上，自然语言理解 NLU 的核心问题其实就是如何从语言文字的表象符号中抽取出来蕴含在文字背后的真实意义，并将其用计算机能够读懂的方式表征出来。当然这通常对应的是数学语言，表征是如此重要，以至于 2012 年的时候 Yoshua Bengio 作为第一作者发表了一篇表征学习的综述 Representation Learning: A Review and New Perspectives，并随后在 2013 年和深度学习三大巨头的另一位巨头 Yann LeCun 牵头创办 ICLR，这一会议至今才过去 5 年时间，如今已是 AI 领域最负盛名的顶级会议之一。可以说，探究 NLP 或 NLU 的历史，同样也是探究文本如何更有效表征的历史。</p>
<p>1954 年 Harris 提出分布假说（distributional hypothesis），这一假说认为：上下文相似的词，其语义也相似，1957 年 Firth 对分布假说进行了进一步阐述和明确：词的语义由其上下文决定（a word is characterized by the company it keeps），30 年后，深度学习 Hinton 也于 1986 年尝试过词的分布式表示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>时间</th>
<th>词语表示的特征</th>
<th>论文</th>
<th>解析</th>
</tr>
</thead>
<tbody>
<tr>
<td>-2003</td>
<td>没有统一表示词语的方法，一般采取统计分析模式</td>
<td></td>
<td>NLP 中并没有一个统一的方法去表示一段文本，各位前辈和大师们发明了许多的方法：从 one-hot 表示一个词到用 bag-of-words 来表示一段文本，从 k-shingles 把一段文本切分成一些文字片段，到汉语中用各种序列标注方法将文本按语义进行分割，从 tf-idf 中用频率的手段来表征词语的重要性，到 text-rank 中借鉴 page-rank 的方法来表征词语的权重，从基于 SVD 纯数学分解词文档矩阵的 LSA，到 pLSA 中用概率手段来表征文档形成过程并将词文档矩阵的求解结果赋予概率含义，再到 LDA 中引入两个共轭分布从而完美引入先验……</td>
</tr>
<tr>
<td>2003</td>
<td>词语的分布式表示</td>
<td>A Neural Probabilistic Language Model</td>
<td>Bengio 在他的经典论文 A Neural Probabilistic Language Model 中，首次将深度学习的思想融入到语言模型中，并发现将训练得到的 NNLM（Neural Net Language Model，神经网络语言模型）模型的第一层参数当做词的分布式表征时，能够很好地获取词语之间的相似度。NNLM 的最主要贡献是非常有创见性地将模型的第一层特征映射矩阵当做词的分布式表示，从而可以将一个词表征为一个向量形式，这直接启发了后来的 word2vec 的工作。</td>
</tr>
<tr>
<td>2007</td>
<td></td>
<td></td>
<td>2007 年 Mnih 和 Hinton 提出的 LBL 以及后续的一系列相关模型，省去了 NNLM 中的激活函数，直接把模型变成了一个线性变换，尤其是后来将 Hierarchical Softmax 引入到 LBL 后，训练效率进一步增强，但是表达能力不如 NNLM 这种神经网络的结构。</td>
</tr>
<tr>
<td>2008</td>
<td></td>
<td></td>
<td>2008 年 Collobert 和 Weston 提出的 C&amp;W 模型不再利用语言模型的结构，而是将目标文本片段整体当做输入，然后预测这个片段是真实文本的概率，所以它的工作主要是改变了目标输出。由于输出只是一个概率大小，不再是词典大小，因此训练效率大大提升，但由于使用了这种比较“别致”的目标输出，使得它的词向量表征能力有限。</td>
</tr>
<tr>
<td>2010</td>
<td></td>
<td></td>
<td>2010 年 Mikolov 提出的 RNNLM 主要是为了解决长程依赖关系，时间复杂度问题依然存在。</td>
</tr>
<tr>
<td>2013</td>
<td><a href="https://baike.baidu.com/item/Word2vec/22660840?fr=aladdin" target="_blank" rel="noopener">word2vec</a>;CBOW 模型;Skip-gram 模型</td>
<td><a href="https://arxiv.org/abs/1301.3781" target="_blank" rel="noopener">Efficient estimation of word representations in vector space</a>;<a href="https://arxiv.org/abs/1310.4546" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and their Compositionality</a></td>
<td>其实 word2vec 只是一个工具，背后的模型是 CBOW 或者 Skip-gram，并且使用了 Hierarchical Softmax 或 Negative Sampling 这些训练的优化方法。word2vec 对于前人的优化，主要是两方面的工作：模型的简化和训练技巧的优化。word2vec 的出现，极大促进了 NLP 的发展，尤其是促进了深度学习在 NLP 中的应用（不过有意思的是，word2vec 算法本身其实并不是一个深度模型，它只有两层全连接），利用预训练好的词向量来初始化网络结构的第一层几乎已经成了标配，尤其是在只有少量监督数据的情况下，如果不拿预训练的 embedding 初始化第一层，几乎可以被认为是在蛮干。</td>
</tr>
<tr>
<td>2014</td>
<td>Glove</td>
<td><a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe: Global Vectors for Word Representation</a></td>
<td>它整个的算法框架都是基于矩阵分解的做法来获取词向量的，本质上和诸如 LSA 这种基于 SVD 的矩阵分解方法没有什么不同，只不过 SVD 分解太过于耗时，运算量巨大，相同点是 LSA 也是输入共现矩阵，不过一般主要以词-文档共现矩阵为主，另外，LSA 中的共现矩阵没有做特殊处理，而 GloVe 考虑到了对距离较远的词对做相应的惩罚等等。然而，相比 word2vec，GloVe 却更加充分的利用了词的共现信息，word2vec 中则是直接粗暴的让两个向量的点乘相比其他词的点乘最大，至少在表面上看来似乎是没有用到词的共现信息，不像 GloVe 这里明确的就是拟合词对的共现频率。</td>
</tr>
<tr>
<td>2015</td>
<td>Skip-shoughts</td>
<td><a href="https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf" target="_blank" rel="noopener">Skip-Thought Vectors</a></td>
<td>2015 年，多伦多大学的 Kiros 等人提出了一个很有意思的方法叫 Skip-thoughts。同样也是借鉴了 Skip-gram 的思想，但是和 PV-DBOW 中利用文档来预测词的做法不一样的是，Skip-thoughts 直接在句子间进行预测，也就是将 Skip-gram 中以词为基本单位，替换成了以句子为基本单位，具体做法就是选定一个窗口，遍历其中的句子，然后分别利用当前句子去预测和输出它的上一句和下一句。对于句子的建模利用的 RNN 的 sequence 结构，预测上一个和下一个句子时候，也是利用的一个 sequence 的 RNN 来生成句子中的每一个词，所以这个结构本质上就是一个 Encoder-Decoder 框架，只不过和普通框架不一样的是，Skip-thoughts 有两个 Decoder。</td>
</tr>
<tr>
<td>2016</td>
<td><a href="https://github.com/facebookresearch/fastText" target="_blank" rel="noopener">fasttext</a></td>
<td><a href="https://arxiv.org/abs/1612.03651" target="_blank" rel="noopener">FastText.zip: Compressing text classification models</a></td>
<td>word2vec 和 GloVe 都不需要人工标记的监督数据，只需要语言内部存在的监督信号即可以完成训练。而与此相对应的，fastText 则是利用带有监督标记的文本分类数据完成训练。在输入数据上，CBOW 输入的是一段区间中除去目标词之外的所有其他词的向量加和或平均，而 fastText 为了利用更多的语序信息，将 bag-of-words 变成了 bag-of-features，也就是下图中的输入 x 不再仅仅是一个词，还可以加上 bigram 或者是 trigram 的信息等等。第二个不同在于，CBOW 预测目标是语境中的一个词，而 fastText 预测目标是当前这段输入文本的类别。fastText 最大的特点在于快。</td>
</tr>
<tr>
<td>2017</td>
<td>InferSent 框架</td>
<td><a href="https://arxiv.org/abs/1705.02364" target="_blank" rel="noopener">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</a></td>
<td>除了 Skip-thoughts 和 Quick-thoughts 这两种不需要人工标记数据的模型之外，还有一些从监督数据中学习句子表示的方法。比如 2017 年 Facebook 的研究人员 Conneau 等人提出的 InferSent 框架，它的思想特别简单，先设计一个模型在斯坦福的 SNLI（Stanford Natural Language Inference）数据集上训练，而后将训练好的模型当做特征提取器，以此来获得一个句子的向量表示，再将这个句子的表示应用在新的分类任务上，来评估句子向量的优劣。</td>
</tr>
<tr>
<td>2018_2</td>
<td><a href="https://allennlp.org/elmo" target="_blank" rel="noopener">ELMo</a></td>
<td><a href="https://arxiv.org/abs/1802.05365v2" target="_blank" rel="noopener">Deep contextualized word representations</a></td>
<td>该研究提出了一种新型深度语境化词表征，可对词使用的复杂特征（如句法和语义）和词使用在语言语境中的变化进行建模（即对多义词进行建模）。这些表征可以轻松添加至已有模型，并在 6 个 NLP 问题中显著提高当前最优性能。</td>
</tr>
<tr>
<td>2018_3</td>
<td>Quick thoughts</td>
<td><a href="https://arxiv.org/abs/1803.02893" target="_blank" rel="noopener">An efficient framework for learning sentence representations</a></td>
<td>2018 年的时候，在 Skip-thoughts 的基础上，Google Brain 的 Logeswaran 等人将这一思想做了进一步改进，他们认为 Skip-thoughts 的 Decoder 效率太低，且无法在大规模语料上很好的训练（这是 RNN 结构的通病）。所以他们把 Skip-thoughts 的生成任务改进成为了一个分类任务，具体说来就是把同一个上下文窗口中的句子对标记为正例，把不是出现在同一个上下文窗口中的句子对标记为负例，并将这些句子对输入模型，让模型判断这些句子对是否是同一个上下文窗口中，很明显，这是一个分类任务。可以说，仅仅几个月之后的 BERT 正是利用的这种思路。而这些方法都和 Skip-thoughts 一脉相承。</td>
</tr>
<tr>
<td>2018_3</td>
<td></td>
<td><a href="https://arxiv.org/abs/1804.00079" target="_blank" rel="noopener">Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning</a></td>
<td>提出了利用四种不同的监督任务来联合学习句子的表征，这四种任务分别是：Natural Language Inference，Skip-thougts，Neural Machine Translation 以及 Constituency Parsing 等。</td>
</tr>
<tr>
<td>2018_3</td>
<td></td>
<td><a href="https://arxiv.org/abs/1803.11175" target="_blank" rel="noopener">Universal Sentence Encoder</a></td>
<td>谷歌的 Daniel Cer 等人在论文 Universal Sentence Encoder 中提出的思路基本和 General Purpose Sentence Representation 的工作一样，只不过作者提出了利用 Transformer 和 DAN（上文提到过的和 CBOW 与 fastText 都神似的 Deep Unordered Composition Rivals Syntactic Methods for Text Classification）两种框架作为句子的 Encoder。</td>
</tr>
<tr>
<td>2018_6</td>
<td><a href="https://github.com/openai/finetune-transformer-lm" target="_blank" rel="noopener">openai-GPT</a></td>
<td><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a></td>
<td>OpenAI 最近通过一个与任务无关的可扩展系统在一系列语言任务中获得了当前最优的性能，目前他们已经发布了该系统。OpenAI 表示他们的方法主要结合了两个已存的研究，即 Transformer 和无监督预训练。实验结果提供了非常令人信服的证据，其表明联合监督学习方法和无监督预训练能够得到非常好的性能。这其实是很多研究者过去探索过的领域，OpenAI 也希望他们这次的实验结果能激发更加深入的研究，并在更大和更多的数据集上测试联合监督学习与无监督预训练的性能。</td>
</tr>
<tr>
<td>2018_10</td>
<td><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">Google-BERT</a></td>
<td><a href="https://arxiv.org/abs/1810.04805v1" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></td>
<td>论文介绍了一种新的语言表征模型 BERT，意为来自 Transformer 的双向编码器表征（Bidirectional Encoder Representations from Transformers）。与最近的语言表征模型（Peters et al., 2018; Radford et al., 2018）不同，BERT 旨在基于所有层的左、右语境来预训练深度双向表征。因此，预训练的 BERT 表征可以仅用一个额外的输出层进行微调，进而为很多任务（如问答和语言推断任务）创建当前最优模型，无需对任务特定架构做出大量修改。BERT 的概念很简单，但实验效果很强大。它刷新了 11 个 NLP 任务的当前最优结果，包括将 GLUE 基准提升至 80.4%（7.6% 的绝对改进）、将 MultiNLI 的准确率提高到 86.7%（5.6% 的绝对改进），以及将 SQuAD v1.1 的问答测试 F1 得分提高至 93.2 分（提高 1.5 分）——比人类表现还高出 2 分。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="NLP-的巨人肩膀（下）：从-CoVe-到-BERT"><a href="#NLP-的巨人肩膀（下）：从-CoVe-到-BERT" class="headerlink" title="NLP 的巨人肩膀（下）：从 CoVe 到 BERT"></a><a href="https://www.jiqizhixin.com/articles/2018-12-17-17" target="_blank" rel="noopener">NLP 的巨人肩膀（下）：从 CoVe 到 BERT</a></h2>
    </div>

    
    
    
        
      
        <div id="reward-container">
  <div>本站所有文章和源码均免费开放，如您喜欢，可以请我喝杯咖啡</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.jpg" alt="袁宵 微信支付">
        <p>微信支付</p>
      </div>
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="袁宵 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>袁宵</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yuanxiaosc.github.io/2019/01/07/词嵌入Embedding和语言模型LanguageModel/" title="Embedding 和语言模型 LanguageModel">https://yuanxiaosc.github.io/2019/01/07/词嵌入Embedding和语言模型LanguageModel/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/Embedding/" rel="tag"># Embedding</a>
            
              <a href="/tags/词嵌入/" rel="tag"># 词嵌入</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/01/06/TensorFlow_estimator使用/" rel="next" title="TensorFlow Estimator使用">
                  <i class="fa fa-chevron-left"></i> TensorFlow Estimator使用
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/01/07/主题模型/" rel="prev" title="主题模型 Topic Modeling with LSA, PLSA, LDA and lda2Vec">
                  主题模型 Topic Modeling with LSA, PLSA, LDA and lda2Vec <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#必读语言模型推荐"><span class="nav-number">1.</span> <span class="nav-text">必读语言模型推荐</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#词语嵌入和语言模型基本概念"><span class="nav-number">2.</span> <span class="nav-text">词语嵌入和语言模型基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么要使用Word-Embedding"><span class="nav-number">2.1.</span> <span class="nav-text">为什么要使用Word Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对比表示文本的各种方法"><span class="nav-number">2.2.</span> <span class="nav-text">对比表示文本的各种方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#One-hot-encodings"><span class="nav-number">2.2.1.</span> <span class="nav-text">One-hot encodings</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用唯一编号对每个单词进行编码"><span class="nav-number">2.2.2.</span> <span class="nav-text">使用唯一编号对每个单词进行编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Word-embeddings-词嵌入"><span class="nav-number">2.2.3.</span> <span class="nav-text">Word embeddings 词嵌入</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#词嵌入资源"><span class="nav-number">3.</span> <span class="nav-text">词嵌入资源</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#向量空间模型"><span class="nav-number">3.1.</span> <span class="nav-text">向量空间模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文本问题"><span class="nav-number">3.2.</span> <span class="nav-text">文本问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#词袋模型"><span class="nav-number">3.3.</span> <span class="nav-text">词袋模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#词向量模型"><span class="nav-number">3.4.</span> <span class="nav-text">词向量模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#词向量"><span class="nav-number">3.5.</span> <span class="nav-text">词向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#统计语言模型"><span class="nav-number">3.6.</span> <span class="nav-text">统计语言模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于神经网络的分布表示"><span class="nav-number">3.7.</span> <span class="nav-text">基于神经网络的分布表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-net-language-models"><span class="nav-number">3.8.</span> <span class="nav-text">Neural net language models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#word2vector"><span class="nav-number">3.9.</span> <span class="nav-text">word2vector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#传统词嵌入的过程embedding-lookup"><span class="nav-number">3.10.</span> <span class="nav-text">传统词嵌入的过程embedding_lookup()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#字符嵌入"><span class="nav-number">3.11.</span> <span class="nav-text">字符嵌入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对比Word2vec-Fasttext-Glove-Elmo-Bert-Flair-pre-train-Word-Embedding"><span class="nav-number">3.12.</span> <span class="nav-text">对比Word2vec, Fasttext, Glove, Elmo, Bert, Flair pre-train Word Embedding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#词嵌入-2018历史综述-Bornstein"><span class="nav-number">4.</span> <span class="nav-text">词嵌入-2018历史综述 Bornstein</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#语言模型词嵌入的过程"><span class="nav-number">5.</span> <span class="nav-text">语言模型词嵌入的过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#自然语言处理中的语言模型预训练方法"><span class="nav-number">6.</span> <span class="nav-text">自然语言处理中的语言模型预训练方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度长文：NLP的巨人肩膀（上）"><span class="nav-number">7.</span> <span class="nav-text">深度长文：NLP的巨人肩膀（上）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NLP-的巨人肩膀（下）：从-CoVe-到-BERT"><span class="nav-number">8.</span> <span class="nav-text">NLP 的巨人肩膀（下）：从 CoVe 到 BERT</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.png"
      alt="袁宵">
  <p class="site-author-name" itemprop="name">袁宵</p>
  <div class="site-description" itemprop="description">专注于机器学习前沿论文（技术）研究和应用，欢迎邮件交流。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives">
        
          <span class="site-state-item-count">143</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">127</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/yuanxiaoSC" title="GitHub &rarr; https://github.com/yuanxiaoSC" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:wangzichaochaochao@gmail.com" title="E-Mail &rarr; mailto:wangzichaochaochao@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>
  <div class="cc-license motion-element" itemprop="license">
    
  
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
	  

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">袁宵</span>
</div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5d9c4b1ac4deb418" async="async"></script>
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">全站共 383.1k 字</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/pisces.js?v=7.4.1"></script>

<script src="/js/next-boot.js?v=7.4.1"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>








  <script src="/js/local-search.js?v=7.4.1"></script>





  <script src="//code.tidio.co/ohblyq9gicnjwqem8o1hfoymk3calgui.js"></script>









  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
