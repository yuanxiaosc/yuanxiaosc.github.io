<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.7.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-32x32.ico?v=7.4.1">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico?v=7.4.1">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico?v=7.4.1">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.1" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="baidu-site-verification" content="eYmWT0dEmt">

<link rel="stylesheet" href="/css/main.css?v=7.4.1">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.1',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":true,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="XLNet是一种基于新型广义排列语言建模目标的新型无监督语言表示学习方法。 此外，XLNet采用Transformer-XL作为骨干模型，为涉及长时间环境的语言任务展示了出色的性能。 总体而言，XLNet在各种下游语言任务上实现了最先进的（SOTA）结果，包括问答，自然语言推理，情感分析和文档排名。">
<meta name="keywords" content="XLNet">
<meta property="og:type" content="article">
<meta property="og:title" content="XLNet Generalized Autoregressive Pretraining for Language Understanding">
<meta property="og:url" content="https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/index.html">
<meta property="og:site_name" content="望江人工智库">
<meta property="og:description" content="XLNet是一种基于新型广义排列语言建模目标的新型无监督语言表示学习方法。 此外，XLNet采用Transformer-XL作为骨干模型，为涉及长时间环境的语言任务展示了出色的性能。 总体而言，XLNet在各种下游语言任务上实现了最先进的（SOTA）结果，包括问答，自然语言推理，情感分析和文档排名。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/自回归和自编码模型例子.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/排列语言模型例子.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/排列模型中的因式分解顺序例子.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/双流自注意力例子.png">
<meta property="og:image" content="https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/双流自注意力模型.png">
<meta property="og:updated_time" content="2019-10-11T09:29:50.350Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="XLNet Generalized Autoregressive Pretraining for Language Understanding">
<meta name="twitter:description" content="XLNet是一种基于新型广义排列语言建模目标的新型无监督语言表示学习方法。 此外，XLNet采用Transformer-XL作为骨干模型，为涉及长时间环境的语言任务展示了出色的性能。 总体而言，XLNet在各种下游语言任务上实现了最先进的（SOTA）结果，包括问答，自然语言推理，情感分析和文档排名。">
<meta name="twitter:image" content="https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/自回归和自编码模型例子.png">
  <link rel="canonical" href="https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>XLNet Generalized Autoregressive Pretraining for Language Understanding | 望江人工智库</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?359fbde2215e8ede98cdd58478ab2c53";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">望江人工智库</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">TF-KMP</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="fa fa-search fa-fw"></i>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/yuanxiaosc" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="袁宵">
      <meta itemprop="description" content="专注于机器学习前沿论文（技术）研究和应用，欢迎邮件交流。">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="望江人工智库">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">XLNet Generalized Autoregressive Pretraining for Language Understanding

          
        </h2>

        <div class="post-meta">
		  	  
			  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
			   

              
                
              

              <time title="创建时间：2019-07-03 10:30:00" itemprop="dateCreated datePublished" datetime="2019-07-03T10:30:00+08:00">2019-07-03</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-10-11 17:29:50" itemprop="dateModified" datetime="2019-10-11T17:29:50+08:00">2019-10-11</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文/" itemprop="url" rel="index"><span itemprop="name">论文</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文/语言模型/" itemprop="url" rel="index"><span itemprop="name">语言模型</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>XLNet是一种基于新型广义排列语言建模目标的新型无监督语言表示学习方法。 此外，XLNet采用Transformer-XL作为骨干模型，为涉及长时间环境的语言任务展示了出色的性能。 总体而言，XLNet在各种下游语言任务上实现了最先进的（SOTA）结果，包括问答，自然语言推理，情感分析和文档排名。</p><a id="more"></a>
<p>推荐阅读：<a href="https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding%E7%BF%BB%E8%AF%91/">XLNet论文 全文中译</a></p>
<h2 id="2019-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding"><a href="#2019-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding" class="headerlink" title="2019 XLNet: Generalized Autoregressive Pretraining for Language Understanding"></a>2019 <a href="https://arxiv.org/abs/1906.08237" target="_blank" rel="noopener">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></h2><blockquote>
<p>With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.</p>
<p>由于具有双向上下文建模的能力，像BERT这样基于自动去噪的预训练语言模型比基于自回归的预训练语言模型的性能更好。然而，依赖于使用带掩码损坏的输入，BERT忽略了掩码位置之间的依赖性，进而受到了预训练-微调不一致的影响。根据这些优点和缺点，我们提出了XLNet，一种广义自回归预训练方法，它（1）通过最大化输入序列的因式分解的所有排列的似然函数的期望来学习双向上下文，并且（2）由于其自回归方法，克服了BERT的局限性。此外，XLNet将最先进的自回归模型Transformer-XL的思想整合到预训练中。实验表明，XLNet在20个任务上常大幅度优于BERT的表现，并在18个任务中实现最先进的结果，包括问答、自然语言推理、情感分析和文档排名。</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>说明</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://arxiv.org/abs/1906.08237" target="_blank" rel="noopener">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></td>
<td>原始论文</td>
<td>20190619</td>
</tr>
<tr>
<td><a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener">xlnet</a></td>
<td>官方实现</td>
<td>持续更新</td>
</tr>
<tr>
<td><a href="https://www.jiqizhixin.com/articles/interview-with-Zhilin-Yang-CMU" target="_blank" rel="noopener">他们创造了横扫NLP的XLNet：专访CMU博士杨植麟</a></td>
<td>原作者解读</td>
<td>20190802</td>
</tr>
<tr>
<td><a href="https://www.jiqizhixin.com/articles/2019-06-29-3" target="_blank" rel="noopener">拆解XLNet模型设计，回顾语言表征学习的思想演进</a></td>
<td>追一科技 Tony</td>
<td>20190629</td>
</tr>
<tr>
<td><a href="https://zhuanlan.zhihu.com/p/70257427" target="_blank" rel="noopener">XLNet:运行机制及和Bert的异同比较</a></td>
<td>张俊林，知乎专栏</td>
<td>20190622</td>
</tr>
<tr>
<td><a href="https://spaces.ac.cn/archives/6933" target="_blank" rel="noopener">从语言模型到Seq2Seq：Transformer如戏，全靠Mask</a></td>
<td>张俊林 解读</td>
<td>20190918</td>
</tr>
<tr>
<td>GitHub <a href="https://github.com/yuanxiaosc/XLNet_Paper_Chinese_Translation" target="_blank" rel="noopener">XLNet_Paper_Chinese_Translation</a></td>
<td>XLNet 中文翻译</td>
<td>20191011</td>
</tr>
</tbody>
</table>
</div>
<h3 id="XLNet-的思考与做法"><a href="#XLNet-的思考与做法" class="headerlink" title="XLNet 的思考与做法"></a>XLNet 的思考与做法</h3><p><img src="/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/自回归和自编码模型例子.png" alt=""></p>
<p>如上所示分别为自回归模型与自编码模型，其中黄色块为输入字符，蓝色块为字符的位置。对于自回归语言模型，它希望通过已知的前半句预测后面的词或字。对于自编码语言模型，它希望通过一句话预测被 Mask 掉的字或词，如上所示第 2 个位置的词希望通过第 1、3、5 个词进行预测。</p>
<p><strong>我们需要更好的语言建模任务</strong></p>
<p>以前，最常见的语言模型就是自回归式的了，它的计算效率比较高，且明确地建模了概率密度。但是自回归语言模型有一个缺陷，它只能编码单向语义，不论是从左到右还是从右到左都只是单向语义。这对于下游 NLP 任务来说是致命的，因此也就有了 BERT 那种自编码语言模型。</p>
<p>BERT 通过预测被 Mask 掉的字或词，从而学习到双向语义信息。但这种任务又带来了新问题，它只是建模了近似的概率密度，因为 BERT 假设要预测的词之间是相互独立的，即 Mask 之间相互不影响。此外，自编码语言模型在预训练过程中会使用 MASK 符号，但在下游 NLP 任务中并不会使用，因此这也会造成一定的误差。</p>
<p>为此，杨植麟表示我们需要一种更好的预训练语言任务，从而将上面两类模型的优点结合起来。XLNet 采用了一种新型语言建模任务，它通过随机排列自然语言而预测某个位置可能出现的词。如下图所示为排列语言模型的预测方式：</p>
<p><img src="/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/排列语言模型例子.png" alt=""></p>
<p>如上排列语言示例，因为随机排列是带有位置信息的，所以扰乱顺序并不影响建模效果。随机排列语言后，模型就开始依次预测不同位置的词。</p>
<p>如果我们知道所有词的内容及位置，那么是不是顺序地分解句子并不太重要。相反这种随机的分解顺序还能构建双向语义，因为如上利用「语言」和「喜欢」预测「处理」就利用了上下文的词。如下原论文展示了不同分解顺序预测同一词的差别，如果第一个分解的词的「3」，那么它就只能利用之前的隐藏状态 mem 进行预测。</p>
<p><img src="/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/排列模型中的因式分解顺序例子.png" alt=""></p>
<p>这理解起来其实也非常直观，如果我们知道某些词及词的位置，那么完形填空式地猜某个位置可能出现哪些词也是没问题的。此外，我们可以发现，这种排列语言模型就是传统自回归语言模型的推广，它将自然语言的顺序拆解推广到随机拆解。当然这种随机拆解要保留每个词的原始位置信息，不然就和词袋模型没什么差别了。</p>
<p><strong>我们需要更好的结构</strong></p>
<p>前面我们为预训练语言模型构建了新的任务目标，这里就需要调整 Transformer 以适应任务。如果读者了解一些 Transformer，那么就会知道某个 Token 的内容和位置向量在输入到模型前就已经加在一起了，后续的隐向量同时具有内容和位置的信息。但杨植麟说：「新任务希望在预测下一个词时只能提供位置信息，不能提供内容相关的信息。因此模型希望同时做两件事，首先它希望预测自己到底是哪个字符，其次还要能预测后面的字符是哪个。」</p>
<p><img src="/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/双流自注意力例子.png" alt=""></p>
<p>这两件事是有冲突的，如果模型需要预测位置 2 的「喜欢」，那么肯定不能用该位置的内容向量。但与此同时，位置 2 的完整向量还需要参与位置 5 的预测，且同时不能使用位置 5 的内容向量。</p>
<p>这类似于条件句：<strong>如果模型预测当前词，则只能使用位置向量；如果模型预测后续的词，那么使用位置加内容向量。因此这就像我们既需要标准 Transformer 提供内容向量，又要另一个网络提供对应的位置向量</strong>。</p>
<p>针对这种特性，研究者提出了 Two-Stream Self-Attention，它通过构建两条路径解决这个条件句。如下所示为 Two-Stream 的结构，其中左上角的 a 为 Content 流，左下角的 b 为 Query 流，右边的 c 为排列语言模型的整体建模过程。注：这只是解决“内容-位置”冲突的其中一种方法。</p>
<p><img src="/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/双流自注意力模型.png" alt=""></p>
<p>在 Content 流中，它和标准的 Transformer 是一样的，第 1 个位置的隐藏向量 h_1 同时编码了内容与位置。在 Query 流中，第 1 个位置的隐向量 g_1 只编码了位置信息，但它同时还需要利用其它 Token 的内容隐向量 h_2、h_3 和 h_4，它们都通过 Content 流计算得出。因此，我们可以直观理解为，Query 流就是为了预测当前词，而 Content 流主要为 Query 流提供其它词的内容向量。</p>
<p>上图 c 展示了 XLNet 的完整计算过程，e 和 w 分别是初始化的词向量的 Query 向量。注意排列语言模型的分解顺序是 3、2、4、1，因此 Content 流的 Mask 第一行全都是红色、第二行中间两个是红色，这表明 h_1 需要用所有词信息、h_2 需要用第 2 和 3 个词的信息。此外，Query 流的对角线都是空的，表示它们不能使用自己的内容向量 h。</p>

    </div>

    
    
    
        
      
        <div id="reward-container">
  <div>本站所有文章和源码均免费开放，如您喜欢，可以请我喝杯咖啡</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.jpg" alt="袁宵 微信支付">
        <p>微信支付</p>
      </div>
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="袁宵 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>袁宵</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/" title="XLNet Generalized Autoregressive Pretraining for Language Understanding">https://yuanxiaosc.github.io/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/XLNet/" rel="tag"># XLNet</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/07/03/XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding翻译/" rel="next" title="XLNet Generalized Autoregressive Pretraining for Language Understanding 翻译">
                  <i class="fa fa-chevron-left"></i> XLNet Generalized Autoregressive Pretraining for Language Understanding 翻译
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/07/11/短视频分类技术/" rel="prev" title="短视频分类技术">
                  短视频分类技术 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding"><span class="nav-number">1.</span> <span class="nav-text">2019 XLNet: Generalized Autoregressive Pretraining for Language Understanding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#XLNet-的思考与做法"><span class="nav-number">1.1.</span> <span class="nav-text">XLNet 的思考与做法</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.png"
      alt="袁宵">
  <p class="site-author-name" itemprop="name">袁宵</p>
  <div class="site-description" itemprop="description">专注于机器学习前沿论文（技术）研究和应用，欢迎邮件交流。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives">
        
          <span class="site-state-item-count">139</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">130</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/yuanxiaoSC" title="GitHub &rarr; https://github.com/yuanxiaoSC" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:wangzichaochaochao@gmail.com" title="E-Mail &rarr; mailto:wangzichaochaochao@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>
  <div class="cc-license motion-element" itemprop="license">
    
  
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
	  

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">袁宵</span>
</div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5d9c4b1ac4deb418" async="async"></script>
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">全站共 393.8k 字</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.1"></script><script src="/js/motion.js?v=7.4.1"></script>
<script src="/js/schemes/pisces.js?v=7.4.1"></script>

<script src="/js/next-boot.js?v=7.4.1"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>








  <script src="/js/local-search.js?v=7.4.1"></script>














  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
